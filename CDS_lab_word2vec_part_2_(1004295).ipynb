{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDS lab word2vec part 2 (1004295)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAA2XCLSQInJ"
      },
      "source": [
        "## Classification with word2vec \n",
        "\n",
        "-- Prof. Dorien Herremans\n",
        "\n",
        "In this second part of the lab, we will be tackling a classification problem by first loading word embeddings and feeding those in a simple classifier. We compare this to naive alternative approaches. \n",
        "\n",
        "During this tutorial, you will need some of the following libraries, let's install them first if you don't have them: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud93NwIrl0Qj"
      },
      "source": [
        "#STUDENT NUMBER: \n",
        "\n",
        "1004295"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0AHw5vB1yOT"
      },
      "source": [
        "# Use this to install libraries if you find them missing on your system: \n",
        "# !pip install bs4 \n",
        "# !pip install sklearn\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yp5xfI15HJ"
      },
      "source": [
        "Now we can import some libraries that we will use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCtMQEnRQjhV"
      },
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "import lxml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368EVM3_QInK"
      },
      "source": [
        "### TFIDF with logistic regression\n",
        "\n",
        "#### Preparing the dataset\n",
        "\n",
        "The classification problem at hand is to predict the tag that belongs to a Stack Overflow post. By the way, if you are not familiar with Stack Overflow, do check it out, it is a tremendous help when facing any coding issues. The data from Google BigQuery is available at the github below. If the link does not work you may have to download it manually from github then upload to Colab:\n",
        "\n",
        "https://github.com/dorienh/class_materials/blob/main/datasets/stack-overflow-data.csv\n",
        " \n",
        " We can read it directly into a pandas dataframe. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBLM0vEjyarI"
      },
      "source": [
        "url = \"https://github.com/dorienh/class_materials/blob/main/datasets/stack-overflow-data.csv?raw=true\"\n",
        "\n",
        "df = pd.read_csv(url, encoding = 'latin-1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzTr6hCQInM"
      },
      "source": [
        "Let's start by having a look at our data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3758TYmIQInQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "a91fe2bf-780b-4f7e-e9a0-a78aad7f5f48"
      },
      "source": [
        "# only keep data that has a tag (is labeled): \n",
        "df = df[pd.notnull(df['tags'])]\n",
        "\n",
        "# display first ten rows:\n",
        "df.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is causing this behavior  in our c# datet...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>have dynamic html load as if it was in an ifra...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to convert a float value in to min:sec  i ...</td>\n",
              "      <td>objective-c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
              "      <td>.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trying to calculate and print the mean and its...</td>\n",
              "      <td>python</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how to give alias name for my website  i have ...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>window.open() returns null in angularjs  it wo...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>identifying server timeout quickly in iphone  ...</td>\n",
              "      <td>iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>unknown method key  error in rails 2.3.8 unit ...</td>\n",
              "      <td>ruby-on-rails</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>from the include  how to show and hide the con...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                post           tags\n",
              "0  what is causing this behavior  in our c# datet...             c#\n",
              "1  have dynamic html load as if it was in an ifra...        asp.net\n",
              "2  how to convert a float value in to min:sec  i ...    objective-c\n",
              "3  .net framework 4 redistributable  just wonderi...           .net\n",
              "4  trying to calculate and print the mean and its...         python\n",
              "5  how to give alias name for my website  i have ...        asp.net\n",
              "6  window.open() returns null in angularjs  it wo...      angularjs\n",
              "7  identifying server timeout quickly in iphone  ...         iphone\n",
              "8  unknown method key  error in rails 2.3.8 unit ...  ruby-on-rails\n",
              "9  from the include  how to show and hide the con...      angularjs"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ECXgs-xn9p"
      },
      "source": [
        "Our task: predict the tag based on the post content. \n",
        "\n",
        "The size of our word embedding will be chosen based on how many unique words are in the dataset (meaning in the article text or posts): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvAXbtE5QInW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7ac8c4-ea67-4cdf-c55b-ad91499341c8"
      },
      "source": [
        "# Count the number of words: \n",
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10286120"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSifacVqQIna"
      },
      "source": [
        "We have over 10 million words in the data. That's a lot! \n",
        "\n",
        "\n",
        "Let's visualise our dataset: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-DdZVX9QInb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "02b54abd-d4a4-4493-9baa-56ae2c3630c3"
      },
      "source": [
        "# visualising dataset\n",
        "plt.figure(figsize=(10,4))\n",
        "df.tags.value_counts().plot(kind='bar');"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEuCAYAAABbHsznAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZykVXn28d/FIiCKooyIwAAiYHABZEBUNLhEgQQQExFUJIiOCyYQjUY0CYjxNXFN4H0lYkBBEQQBRYMCEmVREIZFdsKAEGdkCyggKApc7x/nFF3T090z0/2c6uma6/v59GeqTlU991PT1VV3neU+sk1EREREtLPSdJ9ARERExLBLwhURERHRWBKuiIiIiMaScEVEREQ0loQrIiIiorEkXBERERGNrTLdJ7Ak66yzjjfeeOPpPo2IiIiIJbrsssv+1/as0e3LfcK18cYbM2/evOk+jYiIiIglknTbWO0ZUoyIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGlphwSdpQ0g8lXSfpWkkH1fanSTpH0k3137VruyQdIWm+pKskvajvWPvV+98kab92TysiIiJi+bE0PVyPAB+wvSWwA3CgpC2BDwPn2t4MOLdeB9gF2Kz+zAWOgpKgAYcCLwa2Bw7tJWkRERERw2yJCZft221fXi8/AFwPrA/sARxX73Yc8Pp6eQ/geBcXA0+VtB7wOuAc2/fa/hVwDrBzp88mIiIiYjm0TIVPJW0MbAP8FFjX9u31pjuAdevl9YFf9D1sQW0br32sOHMpvWPMnj173PPZ+MP/uSyn/7hb//lPl/kxg4yVeImXeCtOvGF+bomXeIk3YqknzUt6EnAqcLDt+/tvs23Ayxx9HLaPtj3H9pxZsxarjh8RERExoyxVwiVpVUqydYLt02rznXWokPrvXbV9IbBh38M3qG3jtUdEREQMtaVZpSjgGOB625/ru+kMoLfScD/g233tb6urFXcA7qtDj2cBr5W0dp0s/9raFhERETHUlmYO18uAfYGrJV1Z2z4C/DNwsqQDgNuAveptZwK7AvOBh4D9AWzfK+njwKX1fofbvreTZxERERGxHFtiwmX7QkDj3PzqMe5v4MBxjnUscOyynGBERETETJdK8xERERGNJeGKiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGknBFRERENJaEKyIiIqKxJSZcko6VdJeka/raviHpyvpzq6Qra/vGkn7bd9u/9z1mW0lXS5ov6QhJavOUIiIiIpYvqyzFfb4C/F/g+F6D7Tf1Lkv6LHBf3/1vtr31GMc5Cngn8FPgTGBn4HvLfsoRERERM8sSe7hsnw/cO9ZttZdqL+DEiY4haT1gLdsX2zYleXv9sp9uRERExMwz1TlcLwfutH1TX9smkq6QdJ6kl9e29YEFffdZUNsiIiIiht7SDClOZB8W7d26HZht+x5J2wLfkvS8ZT2opLnAXIDZs2dP8RQjIiIipteke7gkrQK8AfhGr832w7bvqZcvA24GNgcWAhv0PXyD2jYm20fbnmN7zqxZsyZ7ihERERHLhakMKb4GuMH240OFkmZJWrlefjawGXCL7duB+yXtUOd9vQ349hRiR0RERMwYS1MW4kTgImALSQskHVBv2pvFJ8u/Ariqlon4JvBu270J9+8F/gOYT+n5ygrFiIiIWCEscQ6X7X3Gaf/LMdpOBU4d5/7zgOcv4/lFREREzHipNB8RERHRWBKuiIiIiMaScEVEREQ0loQrIiIiorEkXBERERGNJeGKiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0dgSEy5Jx0q6S9I1fW2HSVoo6cr6s2vfbYdImi/pRkmv62vfubbNl/Th7p9KRERExPJpaXq4vgLsPEb7521vXX/OBJC0JbA38Lz6mC9IWlnSysD/A3YBtgT2qfeNiIiIGHqrLOkOts+XtPFSHm8P4CTbDwM/lzQf2L7eNt/2LQCSTqr3vW6ZzzgiIiJihpnKHK73SbqqDjmuXdvWB37Rd58FtW289oiIiIihN9mE6yhgU2Br4Hbgs52dESBprqR5kubdfffdXR46IiIiYuAmlXDZvtP2o7YfA77EyLDhQmDDvrtuUNvGax/v+EfbnmN7zqxZsyZzihERERHLjUklXJLW67u6J9BbwXgGsLek1SRtAmwGXAJcCmwmaRNJT6BMrD9j8qcdERERMXMscdK8pBOBnYB1JC0ADgV2krQ1YOBW4F0Atq+VdDJlMvwjwIG2H63HeR9wFrAycKztazt/NhERERHLoaVZpbjPGM3HTHD/TwCfGKP9TODMZTq7iIiIiCGQSvMRERERjSXhioiIiGgsCVdEREREY0m4IiIiIhpLwhURERHRWBKuiIiIiMaScEVEREQ0loQrIiIiorEkXBERERGNJeGKiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0tMeGSdKykuyRd09f2aUk3SLpK0umSnlrbN5b0W0lX1p9/73vMtpKuljRf0hGS1OYpRURERCxflqaH6yvAzqPazgGeb/uFwH8Dh/TddrPtrevPu/vajwLeCWxWf0YfMyIiImIoLTHhsn0+cO+otrNtP1KvXgxsMNExJK0HrGX7YtsGjgdeP7lTjoiIiJhZupjD9Xbge33XN5F0haTzJL28tq0PLOi7z4LaFhERETH0VpnKgyV9FHgEOKE23Q7Mtn2PpG2Bb0l63iSOOxeYCzB79uypnGJERETEtJt0D5ekvwT+DHhLHSbE9sO276mXLwNuBjYHFrLosOMGtW1Mto+2Pcf2nFmzZk32FCMiIiKWC5NKuCTtDHwI2N32Q33tsyStXC8/mzI5/hbbtwP3S9qhrk58G/DtKZ99RERExAywxCFFSScCOwHrSFoAHEpZlbgacE6t7nBxXZH4CuBwSX8AHgPebbs34f69lBWPa1DmfPXP+4qIiIgYWktMuGzvM0bzMePc91Tg1HFumwc8f5nOLiIiImIIpNJ8RERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGknBFRERENJaEKyIiIqKxJFwRERERjSXhioiIiGgsCVdEREREY0m4IiIiIhpLwhURERHRWBKuiIiIiMaScEVEREQ0loQrIiIiorEkXBERERGNJeGKiIiIaCwJV0RERERjS5VwSTpW0l2Srulre5qkcyTdVP9du7ZL0hGS5ku6StKL+h6zX73/TZL26/7pRERERCx/lraH6yvAzqPaPgyca3sz4Nx6HWAXYLP6Mxc4CkqCBhwKvBjYHji0l6RFREREDLOlSrhsnw/cO6p5D+C4evk44PV97ce7uBh4qqT1gNcB59i+1/avgHNYPImLiIiIGDpTmcO1ru3b6+U7gHXr5fWBX/Tdb0FtG689IiIiYqh1MmnetgF3cSwASXMlzZM07+677+7qsBERERHTYioJ1511qJD67121fSGwYd/9Nqht47UvxvbRtufYnjNr1qwpnGJERETE9JtKwnUG0FtpuB/w7b72t9XVijsA99Whx7OA10pau06Wf21ti4iIiBhqqyzNnSSdCOwErCNpAWW14T8DJ0s6ALgN2Kve/UxgV2A+8BCwP4DteyV9HLi03u9w26Mn4kdEREQMnaVKuGzvM85Nrx7jvgYOHOc4xwLHLvXZRURERAyBVJqPiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGknBFRERENJaEKyIiIqKxJFwRERERjSXhioiIiGhs0gmXpC0kXdn3c7+kgyUdJmlhX/uufY85RNJ8STdKel03TyEiIiJi+bbKZB9o+0ZgawBJKwMLgdOB/YHP2/5M//0lbQnsDTwPeBbwA0mb2350sucQERERMRN0NaT4auBm27dNcJ89gJNsP2z758B8YPuO4kdEREQst7pKuPYGTuy7/j5JV0k6VtLatW194Bd991lQ2yIiIiKG2pQTLklPAHYHTqlNRwGbUoYbbwc+O4ljzpU0T9K8u+++e6qnGBERETGtuujh2gW43PadALbvtP2o7ceALzEybLgQ2LDvcRvUtsXYPtr2HNtzZs2a1cEpRkREREyfLhKufegbTpS0Xt9tewLX1MtnAHtLWk3SJsBmwCUdxI+IiIhYrk16lSKApDWBPwHe1df8KUlbAwZu7d1m+1pJJwPXAY8AB2aFYkRERKwIppRw2X4QePqotn0nuP8ngE9MJWZERETETJNK8xERERGNJeGKiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGknBFRERENJaEKyIiIqKxJFwRERERjU054ZJ0q6SrJV0paV5te5qkcyTdVP9du7ZL0hGS5ku6StKLpho/IiIiYnnXVQ/XK21vbXtOvf5h4FzbmwHn1usAuwCb1Z+5wFEdxY+IiIhYbrUaUtwDOK5ePg54fV/78S4uBp4qab1G5xARERGxXOgi4TJwtqTLJM2tbevavr1evgNYt15eH/hF32MX1LaIiIiIobVKB8fY0fZCSc8AzpF0Q/+Nti3Jy3LAmrjNBZg9e3YHpxgRERExfabcw2V7Yf33LuB0YHvgzt5QYf33rnr3hcCGfQ/foLaNPubRtufYnjNr1qypnmJERETEtJpSwiVpTUlP7l0GXgtcA5wB7Ffvth/w7Xr5DOBtdbXiDsB9fUOPEREREUNpqkOK6wKnS+od6+u2vy/pUuBkSQcAtwF71fufCewKzAceAvafYvyIiIiI5d6UEi7btwBbjdF+D/DqMdoNHDiVmBEREREzTSrNR0RERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGNJuCIiIiIaS8IVERER0VgSroiIiIjGknBFRERENJaEKyIiIqKxJFwRERERjSXhioiIiGgsCVdEREREY0m4IiIiIhpLwhURERHRWBKuiIiIiMaScEVEREQ0NumES9KGkn4o6TpJ10o6qLYfJmmhpCvrz659jzlE0nxJN0p6XRdPICIiImJ5t8oUHvsI8AHbl0t6MnCZpHPqbZ+3/Zn+O0vaEtgbeB7wLOAHkja3/egUziEiIiJiuTfpHi7bt9u+vF5+ALgeWH+Ch+wBnGT7Yds/B+YD2082fkRERMRM0ckcLkkbA9sAP61N75N0laRjJa1d29YHftH3sAWMk6BJmitpnqR5d999dxenGBERETFtppxwSXoScCpwsO37gaOATYGtgduBzy7rMW0fbXuO7TmzZs2a6ilGRERETKspJVySVqUkWyfYPg3A9p22H7X9GPAlRoYNFwIb9j18g9oWERERMdSmskpRwDHA9bY/19e+Xt/d9gSuqZfPAPaWtJqkTYDNgEsmGz8iIiJippjKKsWXAfsCV0u6srZ9BNhH0taAgVuBdwHYvlbSycB1lBWOB2aFYkRERKwIJp1w2b4Q0Bg3nTnBYz4BfGKyMSMiIiJmolSaj4iIiGgsCVdEREREY0m4IiIiIhpLwhURERHRWBKuiIiIiMaScEVEREQ0loQrIiIiorEkXBERERGNJeGKiIiIaCwJV0RERERjSbgiIiIiGkvCFREREdFYEq6IiIiIxpJwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhobOAJl6SdJd0oab6kDw86fkRERMSgDTThkrQy8P+AXYAtgX0kbTnIc4iIiIgYtEH3cG0PzLd9i+3fAycBewz4HCIiIiIGSrYHF0z6C2Bn2++o1/cFXmz7faPuNxeYW69uAdw4iXDrAP87hdNdXmMlXuIl3ooTb5ifW+Il3rDG28j2rNGNq0z9fLpn+2jg6KkcQ9I823M6OqXlJlbiJV7irTjxhvm5JV7irWjxBj2kuBDYsO/6BrUtIiIiYmgNOuG6FNhM0iaSngDsDZwx4HOIiIiIGKiBDinafkTS+4CzgJWBY21f2yjclIYkl+NYiZd4ibfixBvm55Z4ibdCxRvopPmIiIiIFVEqzUdEREQ0loQrIiIiorEkXBERMRCSdpOUz51YIeWFPwmS/mVp2iL6SXqGpNm9n+k+n1g+STpO0lP7rq8t6djpPKcOvQm4SdKnJD13uk8mop+k1ZambbKGJuGSdM4Yb1JnNQr3J2O07dIo1iIkPa/x8Z820U/DuJv2XtiSdpL01/2/zwbxXlZfM/8t6RZJP5d0S6NYu0u6Cfg5cB5wK/C9FrFGxX1mjb2bpGc2inG1pKvG+Lla0lUdx5qW12aN/TJJa9bLb5X0OUkbNQr3Qtu/7l2x/Stgm0axAJC0o6T96+VZkjZpEcf2WynP5WbgK5IukjRX0pNbxAOQtE6rY4+Kc6SkI8b7aRj3U5LWkrSqpHMl3S3prTM9Vl/MNy5NW0cuWsq2SVkuK81P0jqj36QkPaPLAJLeA7wXePaoD5MnAz/uMtYEvgq8qOHxLwMMaIzbDDy7UdxTgTmSnkNZivtt4OvAro3iHQP8DeX5PtooRs/HgR2AH9jeRtIrgdZvUu8A/hH4L8rv8khJh9vuuqfkzzo+3kT6X5uzgV/Vy08F/gdokiRURwFbSdoK+ADwH8DxwB83iLWSpLVrokVNJpu9V0s6FJhD2Ubty8CqwNeAl7WIZ/t+Sd8E1gAOBvYEPijpCNtHdhVH0kq2HwPOpr5nSjrI9r91FWOUeY2OuySvtf0hSXtSvsy9ATif8jucybF6DgFOWYq2SatfSNcH1pC0DSOff2sBT+wqzjAlXI9Jmm37fwDqt8+ua158ndIz8Ungw33tD9i+t+NY4xkrEeqM7ZYfWhN5rNZp2xM40vaRkq5oGO8+2817mao/2L5H0kr1Q+CHkv61ccwPAtvYvgdA0tOBnwCdJly2b+u/LmktGr2v9F6bkr4EnG77zHp9F+D1LWL2ecS2Je0B/F/bx0g6oFGszwIXSep9oLwR+ESjWFASnm2AywFs/7JVj1P9//tL4DmUhHV723dJeiJwHdBZwgWcJ+lB4JmSdgauBvYDmiRcto9rcdyl0Pt7+1PgFNv3Sc0+JgYWq/5d7wqsP6qHcC3gkY7DvY7yutwA+Fxf+/3AR7oKMkwJ10eBCyWdR0lKXs7IBtidsH0fcB+wj6QXATtSkrofA80SrvoNtPfNfl1J/9h3Toc3ijlhL5rtyzsO+QdJ+1DeEHerbat2HKPfDyV9GjgNeLjX2OB5Afxa0pMo3wRPkHQX8GCDOP3uAR7ou/5AbWtC0ruAjwG/Y+SLTqse0R1sv7N3xfb3JH2qQZx+D0g6BNgXeLnKxO8mr0/bx0uaB7yqNr3B9nUtYlW/r8mkAXpDp43sCXze9vn9jbYf6jqBtf3yOi3hMmA74B3A5pJOAs6zfVSX8XokfYcJvuzb3r3jkN+VdAPwW+A9kmZR/g5bGGSsX1J6DXen/A57HqCMTnSmJsvHSfpz26d2eex+Q1X4tI7V71CvXmy7ya7ikv4B2IvyYQ3l2/Uptv+pUbz9+q4eThkqAtp9q5J0MaUb/ipKovdCyov/dyWsXzXBwycTb0vg3cBFtk+sc0j2st1kMYKkH47R3PnzqrHWpLxBrQS8BXgKcEKv96kFSccDL6AMzRrYg/K7vArA9ufGf/Sk4t0EvKTV39yoWGcBFzAyjPEW4BW2X9cw5jOBNwOX2r5AZdHDTraPbxVzUCT9LbAZZW7qJ4G3A1/vcnivxlmZMqz+yi6PO0G8cyi9um+m9KT9qvaa70F5vTQZBpP0b8AzGXl97gPcCXwLwPZ5DWI+jdJr/2jtLVzL9h1dxxl0rBpvVUrn0GzbN7aKU2M9k9Kb/Czbu9TPpZfYPqaT4w9TwtUj6TDbhzU8/o3AVrZ/V6+vAVxpe4tWMftiX2675RyuXpzTgENtX12vPx84zPZfDCD22sCGtjuddD1dJL0f+IbtgW3UXntFx2X7Yx3H+z6lJ+ahLo87TqynAYcCr6hN5wMfaz2sL2ldSk8JwCW272oZb5Ak/QnwWsqXq7Nsn9MozrmU18l9LY4/KtYTgZdQEp95wLqUocyPAxfYbjLnStI823OW1NZBnDdMdLvt0ya6fRljvcr2f40T05QRngttdz4fVtJuwGeAJ9jeRNLWwOENegqR9D3KPMaP2t5K0irAFbZf0MXxh2lIsd/uwGENj/9LYHVGulJXAwb1Ydp0DlefLXrJFoDtayT9Uatgkn5E+b2tQuk+vkvSj22/v1G8p7Doh/Z5lD/iFh8ETwbOlnQv8A1Kb+idDeI8ruuEaikcAvxE0k9ZdIj2r7sOVBOrg7o+7kQk7QV8GvgRI4sQPmj7m4M8j1ZqgtUkyRrlN8DVtffp8WH1Rq+Th4BzJd1hezcoq2qBX1CmLrSa5L6mpGfbvqXGfDbQYph2twluMyMjMF34Y8oCnPFiPh34e8ZewT9VhwHbU/72sH2lGq2ipSy+O7lOH+jt/9xZEjmsCVfrpOQ+4Nr6pmHKi+yS3sS+Fm8efV7d8Nj9rpL0Hyw6bNOyx+kpdfXSO4DjbR+qjssKjHIscA1laBjK3JwvU1bddKomPx+T9EJKHaLzJC2w/ZquY/VI2hz4W2Bj+v7OWwyZVl+kvCFfDTzWIoCkf7V98HhzZFp84+3zUWC7Xq9WnbvyA2DGJ1y11+JfgGdQ3jtFGV5fq0G40+g2EVgaf953+cKaJLf8vR0M/EgjZWY2puP5xAC29+/6mBPEOrTOW/ye7ZPHuo+kTobdxvCHMSbntxqae7AuMOrNZ9yB8nnfiWFNuLZtfPzT60/PjxrHe9wAV0PuD7yHkZ6E8ylL41tZRdJ6lAToow3j9Gxqu/+N+GOSrmwc8y7gDsrk9U5LlozhFODfKeULWpe9AFi1VW9kn6/Wfz/TOM5YVho1hHgPw1PH8FPAbravbx3I9nF1Ckbz+Th9DlUpB/Fr2++pUxY+a/vtjeKtBTyfUqZkd+ClQOdzGyW91fbX6pSFxXQ9T9P2Y5I+BIyZcNlutWr3WklvBlaWtBnw15S5eS28HzgD2FTSj4FZQGfTaIYm4arf6I8C1rX9/NqbsHuLiezTuPwXSaeOShSaqPPTPg98vs6Z2aA3Z62RjwFnUb6BXlq74W9qGO+3kna0fSGUwpaUie2dk/ReSiI5i5IIvbPxqjMoZQxaJsijfU/SXOA7LDqk2NkXBNuX1X/Pk/QEYPN60422/9BVnHF8v07WP7FefxNwZuOYg3LnIJItWHQ+DtB0Pk6fxQrJqtRaauUfbJ+iUlrjVZTnexTw4o7j9IYpmxWNHcMP6iKLb7DokHDLjoC/onwJf5jy93cWZR5e52xfLumPKTXpRMfvLUMzaV6lHMQHgS/a3qa2XWP7+R3GONn2XnUewFhDGi/sKtYE53BF7/k1jvMjRs2pAn5iu9PluH3xjgMO9kixx6bfQlUKWB5PWTEoyqTPv7T9swaxPkmZNN+6B603oRzKt8C7WbzsRZM3Rkk/H6PZtjsvCyFpJ+A4SuFFARsC+3lUqYEGcf+ckWKgF9g+faL7zxR9q+q+xaKvlc6H/iRdRklCftTqfXqMmD+jrCjtLyR7XlcToceId4VLgeNPAlfb/vqg3rdbG+Tf+XSR9FIWn4rRyWrkoenhAp5o+5JR47xdF0frDa8Nsro2Gtl3T8Cqkjasl3Et9NrAoOdUvbD3hgjtv4XWxGorlUKd2L6/YaxDAFR2Pli9r73F7270TgEfGHV7qzfGPxrdAypp9fHuPEWfpVS8vrHG2ZzyzbfpVAKX+jzNavRMo7WAhyirFHu6nnTdM9Z8nCZz/voMupDsQklfpMzt/ReVLcuaDT/Xv7MDgOex6PtL519WPQ2FsQc5H1XSV4FNgSsZmYphypfzKRumhOt/JW3KyGS3vwBu7zKA7dtVasl8xQOqJVMdx8iH6Eb1umpbq0nQg55TNejtTFajTKbdmPJcgTaFZOswyueAZ1F6CjcCrqe8QXbKI9XY16BsQ9UrznsBZU5XKz9h8S2nxmrrwqr9839s/7dKrZ7OSbrQ9o6SHmDRXu2WE8sHapCTrxnsfBxgWgrJ7gXsDHzG9q/r++gHG8b7KnADpVr64ZQFTs2GiFVKBG3Josldy3p0g5yPOgfY0o2G/oYp4TqQsgffcyUtpGwU/Jaug7gUe3tM0lM8gFoyNebjyV3tmm6VZPU7nMHOqRr0t9BvU1afXEbfMEoj/8SA91KkJOX3A70tMd5c2/Ya9xGToEX3IOtPrjrdg2yUy8ZYQdtkib/tHeu/g5wnM1CSNqBsqfP4cClwkO0FDcINbD5Ov5pgtZ432Yv1EH29g7Zvp+Mv/6M8x/YbJe1RFyV8nfI77JxKfb+dKAnXmcAuwIV01AM0jkHOR72GMrze5Pc1THO4Vq7J0JqUFUUPLPFBk4/1bcreY81ryYwRe1BzuJ7uhpXQx4m5JSPfQv+r5bfQ1vNGRsWaZ3tOnUuyTV3t8zPbWzWMeZ3tLZfU1kGc/Sh7kM0BLu276QFKT3Dn85xq7+SBlN47KB8uX7DdOnEeSirlbb7OyCrQtwJvsd2iplJ0TNIltreXdD6lV/sOSmHeFvMnrwa2ohQD3UqlGPDXWr5WJB1GGRk4ncbzUVV2INkauGRUrE4WdQxTD9fPVapdf4NSD6il6agl09Nqp/vRLq5lEr5Mqb3SPDMf5LdQSpHOF7ivuGtDvb0UL2BweyleLmkH2xcDSHoxDXqBPLIH2VspQ24bM/K+8gIWLZ8yZXVI/2e2n8uim8zG5M2y/eW+61+RdHCLQPUDbawFR4PotR9WR9dFRn9PKWnwJCjG5/UAAA2iSURBVOAfGsX6bf3C+Eid/3oXZdFKS72t7fqHZVvt03pYg2M+bpgSrudSJrMfCBwj6bvASb1l/x37JvA7120M6ofAag3iPK6u4jvI9lfq9da1ZDYHXkPZV+0ISSdTeiz+u1G8gehbYboKsL9KccKHGZmT02Kl6e6UXQkOovQerEUpg9HStpSksjcxfzZwY+/5N3ie+wK/Ai6n3Wa2vSH9GyXNbrhgZEVzT02YeyUv9qHdRud/23d5dco8yq4XN60wVIqR3l/nvp5Pu0UxPfNUNgT/EmU6xm+Ai1oGHGuifp0X1yJW5/tc9huaIcV+NRn5N0q3+MoNjn8x8Brbv6nXnwScbfulXcfqi7nYUOIAhxdfSZkvsybwM+DDtpv+kbUiaaOJbrd9W4exxppw3Vue9RilFMWnbX+hq5h9sQf2PGu8QQ7Rnk8Z0r+ERYf0W9ZyGlr1tXIkZd9BUyax//WgEtrekNggYg0jNdincSnjbkzZuHrge95K+q7tgVQLkHS07U52ChimHi5UCpa9ibJCZB4dTxDus3ov2QKw/RuVjVJbGvQqvqdTemPeRpkT8FeU7uqtKatGBr48uAu9REPSV23v239bXRK875gPnFysCSdc1//jnwCdJ1xdJ1RLYZBDtK2GS1ZI9bUykGRVI3XioJRK2JZSCy8mr3kx0lELYha7zfblXcVaGoNKtqovdnWgoUm4JN0KXEHZduCDtlvOkXmw/0UmaVsaVSnvM+hVfBdRJtHubrt/Y+55klqWFxiURUoy1GHh1ltCLcL2PSpFPGes6Riibd3tv6JR2RfynSxe56jFdIX+OnGPUFaTt9oSZkXxpvrvgX1tXc9x+uwEt7UsTzQt6vw0237AdYeLTo47LEOKktZyw+KVo2JtB5wE/JLyxvFM4E1d/mLGiTvIVXzbAR+h1IzqfxNuXk2/JZVd4D8CrEEp9gjld/h74GjXIqWxdAY8RDu6FtboWDO+JtZ0kPQTyoKOy+irc+RS6DVihVE/946lbJck4NfA27v6bJ/xCZekD9n+lKQjGXv1S5NSDSqFFreoVwexl9tASbqRMsH1GvoqQU/DUFUTkj6Z5GpmkvRxSp2cr1LeFN8CrGf7H6f1xGYoSVfa3npAsd4w0e1usJ3QiqTL+UbjHP9tY7U3Lnw6MCq7qRxo+4J6fUdKyZlOOhqGYUixV1G3SeHDsUh6I/B929dI+nvgRZL+adDj2I3dbfs7030SDW0haVfK77H11iLRrd1H1TA7qtY4S8I1Od+VtKvtQWzGfQDwUkZK97ySMo/xbtptJ7QiaT15fru+y6sDr6asTB6KhAt4tJdsAdi+UFJnq2hnfMLVlxQ8ZPuU/ttqYtRCbzf4HSkvuFa7wU+nQ1WqeZ9L4w1tp8kXgP2BI+u8uC+7b7uYWK49KOktlGF9U8oYtK5rNnT6hmgFfETS74FeT70bDdGuStk65fZ6DutRys0McnuhYXZXy4Pb/qv+67VExEktYw7YeSr7YJ5I+dt4E/Cj3qKBqXaqzPghxR5Jl9t+0ZLaOoo1tLvB90j6GqW22bWMDCm6Yd2vaSHpKZQP7I8Cv6DUl/nasA0RD5O6HP3fKFvRGPgxcLDtW6fvrGJpSLre9h/1XV8JuLa/LWaOOrXmGttbLPHOM0AtzDseT7VA74zv4ZK0C7ArsL6kI/puWot2BfUGuhv8NNluWP6IxlPLMuxLKX9xBXACZbuY/Sj7hcVyqCZWe0z3eQyTOrfq8Y3ObX+rUahzJZ3FSJHVvYEfNIq1QpC0OaUK++gFTp2vHJT0HUbmSq9E2VPx5K7jTBf37Vvcwozv4ZK0FaU21OEsOofjAeCHvbpVHcd8IqXW19W2b6rd4i+wfXbXsaaLpC9TinIOaqudgZJ0OmXRw1cpw4l39N02LYUEY+kMuIzB0JP0BeA5jCRBbwJutn3g+I+aUrw9gZfXq+c3TO5WCHX+4r+z+CrTzlfN11qXPY8At7nNJufTQtJBlO3sHqCMdryIUui7k8/2GZ9w9dS6GQ961HY7Lju3t4r5DMrEQQCGaasRSdcDm1Lq5LTe+mbgas/o8yjDUo9Rdrw/ynazbWmiGylj0C1JNwB/5Pph0GKYb4xdF9R3c9NdF4adpMtsD7SG4LCS9DOXTblfB7ybsj/lV7uamjTjhxT7nE3Z+69XAX6N2tb5djuSdqcUgnsWZZLibOAGRhXTnOF2nu4TaGx/4H6gNwz9ZkpvV6uFFtGdJ9r+u+k+iSEyn/Ie1iv5smFt68x07rqwAviOpPdSNorvX+DUWaX5nnFq4d1HqRLwAdu3dB1zwHpfBHYFjrd9rSRN9IBlMUwJ1yC32/k4sAPwgzp5/pWUeUBDY1jqbU3g+ba37Lv+Q0lDOXw6hAZZxmBF8GTgekmXUD5Mt6fsKHEGDGaPymHYdWEa7Vf//WBfW9eV5nv+FVgAfJ2SnOxNGQm5nFIwdKcGMQfpMklnU7auO0TSk+mrQzlVw5RwDXK7nT/UN4iVJK1k+4eS/rVRrGjjckk72L4YQNKLGWAtt5iSgyhlDB6mlDHoDXen0vzkLBf1y3qlImLZ2B7kvraja+AdXQvn/p2kjwzwPFo5gDInfFVKTbN1gK90dfBhSrgOBk6RtMh2O41i/VrSk4DzgRMk3UXqAM0021I2XO7Nu5sN3NjbG3BY5qoNI9tPVtkEeTP65lDG5GRvypmtlmZ4D/CK2vQj4IuNSts8JGkv4Jv1+l8AvXmvwzAh/O2UL3QbAFdSRrIuAo7s4uBDM2keBrfdjqQ1Kb1nK1G2FXkKcILte1rEi+4Ncg/A6Jakd7D4m+JPbL96Wk9shhljIvvjN5EewxmjFqheFTiuNu1LqZj+jgaxnk2pgfcSymvmYuBvgIXAtrYv7DrmINUv3NsBF9veWtJzgf9je8ItqZb6+MOScNX5Wu8HNrL9TkmbAVvY/m6DWO8HvmF7YdfHjoiJtX5TjJhJeivrltQWSybpUtvbSboSeLHthyVda7uTBXHDNKT4Zcoy8ZfU6wuBU4DOEy7KJNOzJd0LfAM4xfadDeJExOJ+Z/t3kpC0mu0bJA11kd6ICTwqaVPbN8PjvVCPLuExk7IC1MBbULcr+hZwjqRfMbJ6d8qGqYdrnu05/VvstM7yJb2QMk/sz4EFtl/TKlZEFLVo7f6UeZuvAn4FrGp712k9sYhpIOnVlA6HWyjDwRsB+9ueaJuaycZaYWrg1SKvTwG+b/v3XRxzmHq4fi9pDepcBEmb0leTpJG7gDuAe4BnNI4VEYDtPevFw+reZ08Bvj+NpxQxbWyf25tCU5tutN3qs2+FqYHXYjHJMO3/dyjlTXdDSScA5wIfahFI0nsl/ajGeDrwzqxqixg82+fZPqOrb6ARM5Hth21fBbyhYbIFtQZew+MPtaEZUoTHqxXvQOlWvdj2/zaK80nKpPkrWxw/IiJiWUm6vKttaMY5/gPAmpTRo9TAW0YzPuGS9Nw6aXasF5mBe1st8R/mvRQjImJm6Z/D3DDGYjXwUstt6QxDwnW07bl1LsdYng78zPa+HcbcDfgcI3spbgRc39XS0YiIiKUh6QW2r66XV7Ld2VY0Y8RKDbwpmPEJ19KQdLbt13Z4vJ9RVkctspei7QO6ihEREbEkki4AVqNsQXOC7fsaxkoNvCkYmknzklaX9H5Jp0k6VdLBklYH6DLZqv5Qq8o/vpciZd+liIiIgbH9csqOJxtSNl/+uqQ/aRTud7Z/BzxeA4+R1ZGxBMNUFuJ44AFG9jx6M/BV4I0NYvX2UryA7KUYERHTyPZNkv4emAccAWwjScBHbJ/WYaimhUGH3dAMKUq6zvaWS2rrKNYTKRt2CngrsBalK/fermNFRESMpxbg3h/4U+Ac4Bjbl0t6FnCR7Qn3jZ1C3M4Lgw67YerhulzSDrYvBpD0Ykq235neZq/AnYxs9qr67z/VrX4+bfsLXcaNiIgYx5HAMZTerN/2Gm3/svZ6NZGVictuxvdw1Ul8puyWvgXwP/X6RsANLXq4JjiXp1NWbGRMOyIiBkLSE4DnUj77bkyP0/JpGBKu/u7StYGX18vnA79uVYNrgvNZz/btg4wZERErplr5/YvAzZQRl02Ad9n+3rSeWCxmxidcPZIOAt4BnEZ50b0e+JLtIyd8YERExAwl6Qbgz2zPr9c3Bf7T9nOn98xitGFKuK4CXmL7wXp9TcqEwexxGBERQ0nSpba367su4JL+tlg+DNOkeQGP9l1/lJEJ7REREUNDUq/Y6DxJZwInU+ZwvRG4dNpOLMY1TAnXl4GfSjq9Xn89ZeVGRETEsNmt7/KdwB/Xy3cDawz+dGJJhmZIEaBuYL1jvXqB7Sum83wiIiIiYMgSroiIiBWJpC8zUhfycbbfPg2nExMYpiHFiIiIFc13+y6vDuwJ/HKaziUmkB6uiIiIISFpJeBC2y+d7nOJRa003ScQERERndkMeMZ0n0QsLkOKERERM1CtufUo8Ju+5juAv5ueM4qJJOGKiIiYgWxb0nW2nz/d5xJLliHFiIiImesySakqPwNk0nxERMQMVfdSfA5wG/AgZYcVZ1u75U8SroiIiBlK0kZjtdu+bdDnEhNLwhURERHRWOZwRURERDSWhCsiIiKisSRcEREREY0l4YqIiIhoLAlXRERERGP/HzVL5ZSztFXtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UwHhJXMQIng"
      },
      "source": [
        "As you can see, the classes are very well balanced.\n",
        "\n",
        "Now let's have a look at the data of the posts ('post' columns) in more detail: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q3m4GxxQInh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce20e2c0-50b6-43d7-ccc2-67d6c452a2c9"
      },
      "source": [
        "print(df['post'].values[10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when we need interface c# <blockquote>    <strong>possible duplicate:</strong><br>   <a href= https://stackoverflow.com/questions/240152/why-would-i-want-to-use-interfaces >why would i want to use interfaces </a>   <a href= https://stackoverflow.com/questions/9451868/why-i-need-interface >why i need interface </a>    </blockquote>     i want to know where and when to use it     for example    <pre><code>interface idemo {  // function prototype  public void show(); }  // first class using the interface class myclass1 : idemo {  public void show()  {   // function body comes here   response.write( i m in myclass );  }  }  // second class using the interface class myclass2 : idemo {  public void show()   {   // function body comes here   response.write( i m in myclass2 );   response.write( so  what  );  } </code></pre>   these two classes has the same function name with different body. this can be even achieved without interface. then why we need an interface where and when to use it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbSGI0FQQInu"
      },
      "source": [
        "As you can see, the text needs to be cleaned up a bit. Below we use the `nltk` toolkit to remove spaces, html tags, stopwords, symbols etc. We define a function to remove stop words, replace / \\ and other symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydjpKCBQInv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b8a812-5716-4a1f-ed23-20d59d403e84"
      },
      "source": [
        "# note: slower students may wish to skip this step to finish the lab in class\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# load a list of stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, 'html.parser').text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiiOIj3L_NDH"
      },
      "source": [
        "Now we can apply the newly defined function on the column of `df 'post'`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A8nlP0hQInx"
      },
      "source": [
        "df['post'] = df['post'].apply(clean_text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBB_8za_SE5"
      },
      "source": [
        "Let's check the results: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlL2uGKsQIn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9fc193-8e58-4fba-f59d-a7d6c0300abb"
      },
      "source": [
        "print(df['post'].values[10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need interface c# possible duplicate would want use interfaces need interface want know use example interface idemo function prototype public void show first class using interface class myclass1 idemo public void show function body comes responsewrite myclass second class using interface class myclass2 idemo public void show function body comes responsewrite myclass2 responsewrite two classes function name different body even achieved without interface need interface use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTkqUfwzQIn8"
      },
      "source": [
        "This looks a lot better!\n",
        "\n",
        "Now how many unique words do we have in this cleaned up dataset? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oV5baXxQIn8",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d444c10d-7dc5-4062-db9d-5b55ca887353"
      },
      "source": [
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3424194"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpllc3QyQIoA"
      },
      "source": [
        "Now we have over 3 million words to work with, that's 7 million removed tags.\n",
        "\n",
        "Before we start creating classifiers, let's split our dataset 70-30 in a test set (for evaluation) and training set: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylA7e4H_QIoB"
      },
      "source": [
        "X = df.post\n",
        "y = df.tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDHptCZqQIoU"
      },
      "source": [
        "#### Logistic regression\n",
        "\n",
        "Now that we have our features, we can train a classifier to try to predict the tag of a post. We will start with logistic regression and TFIDF representation which provides a nice baseline for this task. \n",
        "\n",
        "To make the vectorizer => transformer => classifier easier to work with, we will use the `Pipeline` class in Scikit-Learn that behaves like a compound classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IMmMZWQIoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee973729-0d6b-4b35-eff9-4b06e3df6766"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# we define a Pipeline, which first represents our features as TFID\n",
        "# Then performs logistic regression\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=1, penalty='l2',\n",
              "                                    random_state=None, solver='lbfgs',\n",
              "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plgMq5lA27-_"
      },
      "source": [
        "How well does it work? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGSVTzWRQIoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb9bfcd-872c-4ee5-843b-63d250a0e917"
      },
      "source": [
        "# to show the computation time: \n",
        "%%time\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.7861666666666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.67      0.64      0.65       613\n",
            "      android       0.91      0.90      0.91       620\n",
            "    angularjs       0.97      0.93      0.95       587\n",
            "      asp.net       0.76      0.75      0.75       586\n",
            "            c       0.78      0.83      0.81       599\n",
            "           c#       0.61      0.59      0.60       589\n",
            "          c++       0.79      0.76      0.77       594\n",
            "          css       0.84      0.87      0.85       610\n",
            "         html       0.70      0.73      0.71       617\n",
            "          ios       0.61      0.59      0.60       587\n",
            "       iphone       0.65      0.63      0.64       611\n",
            "         java       0.83      0.82      0.83       594\n",
            "   javascript       0.77      0.79      0.78       619\n",
            "       jquery       0.85      0.85      0.85       574\n",
            "        mysql       0.82      0.85      0.83       584\n",
            "  objective-c       0.66      0.66      0.66       578\n",
            "          php       0.82      0.83      0.83       591\n",
            "       python       0.93      0.90      0.91       608\n",
            "ruby-on-rails       0.96      0.94      0.95       638\n",
            "          sql       0.79      0.85      0.82       601\n",
            "\n",
            "     accuracy                           0.79     12000\n",
            "    macro avg       0.79      0.79      0.79     12000\n",
            " weighted avg       0.79      0.79      0.79     12000\n",
            "\n",
            "CPU times: user 1.26 s, sys: 12.6 ms, total: 1.28 s\n",
            "Wall time: 1.28 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAAVJw44xn_c"
      },
      "source": [
        "That's quite a good accuracy. Now let's see if we can combine **word2vec** with logistic regression by feeding the new embedded representation to our logistic regression instead of the bag of words representation of TFIDF. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gSX1ysMQIoc"
      },
      "source": [
        "### Word2vec embedding and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubl-sOB8W2f1"
      },
      "source": [
        "Let's load a pretrained word2vec model, and use the embedding representation as input to a simple classifier (i.e. logistic regression). \n",
        "\n",
        "You can use the word2vec model you trained in the first part of the lab (on the Shakespeare text), or load this (quite big, 1.5GB) pretrained word2vec model from Google trained on Google News data. \n",
        "\n",
        "If you load an model you trained yourself, use#\n",
        "`wv = gensim.models.KeyedVectors.load_word2vec_format(\"yourweights.bin.gz\", binary=True)`. We will be loading pretrained weights available in gensim itself:\n",
        "\n",
        "(This may take a while!)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjipngb9QIod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55902a21-3c56-4391-b755-91dbd285aa75"
      },
      "source": [
        "%%time\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", \n",
        "                                                    #  binary=True)\n",
        "\n",
        "wv = gensim.downloader.load('word2vec-google-news-300')\n",
        "wv.init_sims(replace=True)\n",
        "print('Model loaded')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Model loaded\n",
            "CPU times: user 12min 45s, sys: 1min 50s, total: 14min 36s\n",
            "Wall time: 18min 38s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-W7lH8wxn_2"
      },
      "source": [
        "If you are interested how good these pretrained embeddings are, you could try some of the similarity tests we did in part 1 of the lab on the Shakespeare text. Only now we have a larger vocabulary, e.g.:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyy5c5yC6XTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b557b66b-f0ed-46ae-9469-3fdaf7f98546"
      },
      "source": [
        "wv.most_similar('twitter')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Twitter', 0.89089035987854),\n",
              " ('Twitter.com', 0.7536780834197998),\n",
              " ('tweet', 0.7431625723838806),\n",
              " ('tweeting', 0.7161933183670044),\n",
              " ('tweeted', 0.7137226462364197),\n",
              " ('facebook', 0.6988551616668701),\n",
              " ('tweets', 0.6974530816078186),\n",
              " ('Tweeted', 0.6950210928916931),\n",
              " ('Tweet', 0.6875007152557373),\n",
              " ('Tweeting', 0.6845167279243469)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3UmhwMo6f4g"
      },
      "source": [
        "Gensim offers a number of pretrained models for you to choose from (convenient right!). You can check a list of available model like this: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VIeyk0s6gCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70e0f56-b3a9-4822-a233-9c9e21a8e74b"
      },
      "source": [
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC4XwSuzQIoo"
      },
      "source": [
        "As we have multiple words for each post, we will need to somehow combine them. A common way to achieve this is by averaging the\n",
        "word vectors per document. In later classes you can feed the individual words to memory models like LSTM. For a quick solution here, we can use a summation or weighted addition. The function below takes as input a list of words and the word2vec model `wv`. Then it retrieves the vector embeddings for each of the words and averages them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbSLtiwyQIoo"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    # averages a set of words 'words' given their wordvectors 'wv'\n",
        "    \n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    # for each word in the list of words\n",
        "    for word in words:\n",
        "        # if the words are alread vectors, then just append them\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        # if not: first get the vector embedding for the words\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    \n",
        "    if not mean:\n",
        "        # error handling in case mean cannot be calculated\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    # use gensim's method to calculate the mean of all the words appended to mean list\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm_febK-xoAC"
      },
      "source": [
        "Below, we explore a way (slightly different from the method used in part 1 of the lab) to create tokens out of sentences, by using the `nltk` toolkit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlyXtYm1QIos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7102a18-9cc0-4b83-ecc1-8cd001bf020e"
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    # create tokens, a list of words, for each post. This function will do some cleaning based on English language\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeuQn-3GxoAQ"
      },
      "source": [
        "Let's also split the dataset in training and test set like before, and tokenize each of these datasets using the method defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1Wrn9-QIot"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh11CM3ZxoAa"
      },
      "source": [
        "Since we have multiple word vectors per article, we can take multiple approaches (a powerful LSTM approach as we'll see later, or doc2vec as per below, but first we try a naive approach of averaging). We can average the word positions for each post in this new dataset using the functions we defined above and based on our word2vec model `wv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqG34rU6QIoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133729ec-9008-491c-cedd-fc4dd217a40c"
      },
      "source": [
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  del sys.path[0]\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input ['ngrepeat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQa7btkxoAj"
      },
      "source": [
        "Now we have a way to represent our input! This can then be fed to any classifier, like logistic regression: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSSSuFQYQIo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21835f5b-3e49-4055-820a-dd73fcb899d9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
        "y_pred = logreg.predict(X_test_word_average)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fy9vOz44h9p"
      },
      "source": [
        "Let's evaluate how accurate this averaged word2vec representation with logistic regression is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_WzSihIQIo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ffeaca-172e-4e7d-c468-e3461d35a6ee"
      },
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
        "print(classification_report(test.tags, y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.6323333333333333\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.63      0.56      0.59       613\n",
            "      android       0.76      0.76      0.76       620\n",
            "    angularjs       0.64      0.66      0.65       587\n",
            "      asp.net       0.54      0.50      0.52       586\n",
            "            c       0.70      0.73      0.71       599\n",
            "           c#       0.39      0.43      0.41       589\n",
            "          c++       0.63      0.63      0.63       594\n",
            "          css       0.73      0.78      0.75       610\n",
            "         html       0.57      0.62      0.60       617\n",
            "          ios       0.55      0.53      0.54       587\n",
            "       iphone       0.57      0.52      0.54       611\n",
            "         java       0.63      0.59      0.61       594\n",
            "   javascript       0.64      0.61      0.62       619\n",
            "       jquery       0.59      0.57      0.58       574\n",
            "        mysql       0.68      0.73      0.71       584\n",
            "  objective-c       0.41      0.40      0.41       578\n",
            "          php       0.67      0.70      0.68       591\n",
            "       python       0.80      0.76      0.78       608\n",
            "ruby-on-rails       0.83      0.81      0.82       638\n",
            "          sql       0.67      0.71      0.69       601\n",
            "\n",
            "     accuracy                           0.63     12000\n",
            "    macro avg       0.63      0.63      0.63     12000\n",
            " weighted avg       0.63      0.63      0.63     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNXuL03bxoAy"
      },
      "source": [
        "Now you can see that the accuracy went down! Oh no! Why is that? Because we used a very naive approach: averaging our vectors. A better way to approach this would be doc2vec, which learns relationships between documents (posts in this case), instead of words. The accuracy could also improve by using a different classifier instead of logistic regression, or by changing the aggregation strategy and feed it to an LSTM/RNN model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtujiFgzQIpA"
      },
      "source": [
        "## Doc2vec and Logistic Regression (advanced)\n",
        "\n",
        "The idea of word2vec can be extended to documents whereby instead of learning feature representations for words, we learn it for sentences or documents. Doc2Vec extends the idea of word2vec, however words can only capture so much, there are times when we need relationships between documents and not just words.\n",
        "\n",
        "The way to train doc2vec model for our Stack Overflow questions and tags data is very similar to when we trained multi-class text classification with word2vec and logistic regression above.\n",
        "\n",
        "First, we label the sentences. Gensim’s Doc2Vec implementation requires each document/paragraph to have a label associated with it that indicates if it's part of the test or training set. We do this by using the TaggedDocument method. The format will be `TRAIN_i` or `TEST_i` where `i` is a dummy index of the post.\n",
        "\n",
        "First let's import the necessary libraries. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkXdv0A6QIpB"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from gensim.models import doc2vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBov76MXxoA8"
      },
      "source": [
        "Let's start by defining a function that labels our documents in the corpus. We just give them dummy labels TRAIN_i or TEST_i for post i. Given a corpus and labels, we return a variable that includes a label indicating if it's test or training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVzwM8RQIpD"
      },
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lsDfgBnxoBC"
      },
      "source": [
        "Just like above we split our dataset up in test and training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfXghWoJQIpF"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, \n",
        "                                                    test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KYhbHmxoBI"
      },
      "source": [
        "Let's have a look how our data looks at this moment: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321apZFWQIpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09425ed-2aee-4dae-f79e-d3bc38a07afe"
      },
      "source": [
        "all_data[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['fulltext', 'search', 'php', 'pdo', 'returning', 'result', 'searched', 'lot', 'matter', 'find', 'wrong', 'setup', 'trying', 'fulltext', 'search', 'using', 'pdo', 'php', 'get', 'results', 'error', 'messages', 'table', 'contains', 'customer', 'details', 'id', 'int', '11', 'auto_increment', 'name', 'varchar', '150', 'lastname', 'varchar', '150', 'company', 'varchar', '250', 'adress', 'varchar', '150', 'postcode', 'int', '5', 'city', 'varchar', '150', 'email', 'varchar', '250', 'phone', 'varchar', '20', 'orgnr', 'varchar', '15', 'timestamp', 'timestamp', 'current_timestamp', 'run', 'sqlquery', 'alter', 'table', 'system_customer', 'add', 'fulltext', 'name', 'lastname', 'except', 'columns', 'id', 'postcode', 'timestamp', 'signs', 'trouble', 'far', 'idea', 'problem', 'lies', 'db', 'configuration', 'php', 'code', 'goes', 'php', 'sth', 'dbhprepare', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'db_pre', 'customer', 'match', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'search', 'boolean', 'mode', 'bind', 'placeholders', 'sthbindparam', 'search', 'data', 'sthexecute', 'rows', 'sthfetchall', 'testing', 'print_r', 'dbherrorinfo', 'empty', 'rows', 'echo', 'else', 'echo', 'foreach', 'rows', 'row', 'echo', 'tr', 'datahref', 'new_orderphp', 'cid', 'row', 'id', 'echo', 'td', 'row', 'name', 'td', 'echo', 'td', 'row', 'lastname', 'td', 'echo', 'td', 'row', 'company', 'td', 'echo', 'td', 'row', 'phone', 'td', 'echo', 'td', 'row', 'email', 'td', 'echo', 'td', 'date', 'ymd', 'strtotime', 'row', 'timestamp', 'td', 'echo', 'tr', 'echo', 'tried', 'change', 'parameter', 'searchquery', 'string', 'like', 'testcompany', 'somename', 'boolean', 'mode', 'also', 'read', 'word', 'found', '50', 'rows', 'counts', 'common', 'word', 'pretty', 'sure', 'case', 'uses', 'specific', 'words', 'table', 'uses', 'myisam', 'engine', 'get', 'results', 'error', 'messages', 'please', 'help', 'point', 'wrong', 'thank'], tags=['Train_0']),\n",
              " TaggedDocument(words=['select', 'everything', '1', 'table', 'x', 'rows', 'another', 'im', 'making', 'join', 'query', 'like', 'select', 'clothes', 'c', 'join', 'style', 'cstyleid', 'ssylelid', 'clothesid', '19', 'dont', 'want', 'select', 'everything', 'style', 'want', 'select', 'everything', 'clothes', '20', 'rows', 'select', '1', 'row', '10', 'style', 'easyest', 'way', 'without', 'select', 'every', 'row', 'clothes', '20', 'things', 'select', 'like', 'select', 'cid', 'cdescription', 'cname', 'csize', 'cbrand', 'sname', 'clothes', 'c', 'join', 'style', 'cstyleid', 'stsylelid', 'clothesid', '19', 'would', 'fastest', 'way', 'possibillity'], tags=['Train_1']),\n",
              " TaggedDocument(words=['r', 'cannot', 'resolved', 'variable', 'importing', 'project', 'pasting', 'problems', 'details', 'r', 'cannot', 'resolved', 'variable', 'common', 'problem', 'checked', 'res', 'folder', 'done', 'refreshing', 'project', 'cleaning', 'project', 'validate', 'still', 'error', 'resolved', 'help', 'guys', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resdrawableic_launcher_wallpaperpng0', 'error', 'resource', 'entry', 'ic_launcher_wallpaper', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resdrawableic_launcher_wallpaperhtml0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube1xml0', 'error', 'resource', 'entry', 'cube1', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube1html0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2xml0', 'error', 'resource', 'entry', 'cube2', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2html0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2_settingsxml0', 'error', 'resource', 'entry', 'cube2_settings', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2_settingshtml0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesindexhtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesshapeshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesstringshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube1html112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube2html112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube2_settingshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlindexhtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', 'copyright', 'c', '2009', 'google', 'inc', 'licensed', 'apache', 'license', 'version', '20', 'license', 'may', 'use', 'file', 'except', 'compliance', 'license', 'may', 'obtain', 'copy', 'license', 'http', 'wwwapacheorg', 'licenses', 'license20', 'unless', 'required', 'applicable', 'law', 'agreed', 'writing', 'software', 'distributed', 'license', 'distributed', 'basis', 'without', 'warranties', 'conditions', 'kind', 'either', 'express', 'implied', 'see', 'license', 'specific', 'language', 'governing', 'permissions', 'limitations', 'license', 'package', 'comexampleandroidlivecubescube2', 'import', 'comexampleandroidlivecubescube2', 'import', 'androidcontentsharedpreferences', 'import', 'androidosbundle', 'import', 'androidpreferencepreferenceactivity', 'public', 'class', 'cubewallpaper2settings', 'extends', 'preferenceactivity', 'implements', 'sharedpreferencesonsharedpreferencechangelistener', 'override', 'protected', 'void', 'oncreate', 'bundle', 'icicle', 'superoncreate', 'icicle', 'getpreferencemanager', 'setsharedpreferencesname', 'cubewallpaper2shared_prefs_name', 'addpreferencesfromresource', 'rxmlcube2_settings', 'getpreferencemanager', 'getsharedpreferences', 'registeronsharedpreferencechangelistener', 'override', 'protected', 'void', 'onresume', 'superonresume', 'override', 'protected', 'void', 'ondestroy', 'getpreferencemanager', 'getsharedpreferences', 'unregisteronsharedpreferencechangelistener', 'superondestroy', 'public', 'void', 'onsharedpreferencechanged', 'sharedpreferences', 'sharedpreferences', 'string', 'key'], tags=['Train_2']),\n",
              " TaggedDocument(words=['efficient', 'way', 'get', 'values', 'object', 'based', 'id', 'list', 'list', 'users', 'users', 'arrayany', 'id1', 'name', 'id2', 'name', 'b', 'id3', 'name', 'c', 'thisselectedusers', '1', '2', 'get', 'objects', 'users', 'array', 'id', 'found', 'selectusers', 'array'], tags=['Train_3']),\n",
              " TaggedDocument(words=['aspnet', 'limit', 'parameter', 'length', 'querystring', 'problem', 'passing', 'parameters', 'querystring', 'found', 'values', 'null', 'code', 'snippet', 'page1', 'passing', 'parameters', 'responseredirect', 'stringformat', 'requestreservationpageaspx', 'plcname', '0', 'plcindex', '1', 'email', '2', 'form', '3', '4', 'sr', '5', 'comment', '6', 'lblplcnamevaltext', 'index', 'lblemailvaltext', 'datetimeparse', 'lblreqfromvaltext', 'toshortdatestring', 'datetimeparse', 'lblreqtovaltext', 'toshortdatestring', 'lblservreqnumtext', 'lblyourcommentvaltext', 'page2', 'requesting', 'values', 'cmbplcrequestselectedindex', 'converttoint32', 'requestquerystring', 'plcindex', 'txtemailtext', 'converttostring', 'requestquerystring', 'email', 'txtsrtext', 'converttostring', 'requestquerystring', 'sr', 'txtcommenttext', 'converttostring', 'requestquerystring', 'comment', 'txtreqfromdatetext', 'requestquerystring', 'txtreqtodatetext', 'requestquerystring', 'found', 'requestquerystring', 'requestquerystring', 'return', 'null', 'idea'], tags=['Train_4']),\n",
              " TaggedDocument(words=['ruby', 'rails', 'fetch', 'display', 'descendent', 'records', 'parent', 'model', 'many', 'children', 'class', 'band', 'activerecordbase', 'has_many', 'concerts', 'end', 'class', 'concerts', 'activerecordbase', 'belongs_to', 'band', 'end', 'would', 'like', 'display', 'index', 'view', 'figure', 'syntax', 'displaying', 'children', 'records', 'bandseach', 'band', 'band', 'name', 'bandname', 'concerts', 'ul', 'bandsconcertseach', 'concert', 'concertlocation', 'end', 'ul', 'end', 'getting', 'error', 'like', 'undefined', 'method', 'concerts', '#array0x00000102c537f0', 'proper', 'way', 'fetching', 'displaying', 'descendent', 'models'], tags=['Train_5']),\n",
              " TaggedDocument(words=['canceling', 'fade', 'effect', 'tooltip', 'hover', 'need', 'tooltip', 'hyperlinks', 'inside', 'web', 'site', 'wrote', 'code', 'seems', 'working', 'fine', 'expect', 'one', 'problem', 'hover', 'tooltip', 'block', 'fades', 'fades', 'need', 'prevent', 'fadeout', 'hovet', 'somehow', 'tried', 'use', 'stop', 'method', 'work', 'probably', 'something', 'wrong', 'could', 'please', 'help', 'thanks', 'html', 'div', 'id', 'hover', 'div', 'class', 'tooltip', 'href', '#', 'href', '#', 'href', '#', 'div', 'div', 'css', 'body', 'margin', '0', 'padding', '0', 'width', '100', 'height', '100', '#hover', 'position', 'relative', 'width50px', 'height50px', 'background', 'green', 'tooltip', 'position', 'relative', 'width', '45px', 'top', '80px', 'height', '20px', 'border', '1px', 'solid', 'black', 'padding', '5px', 'display', 'inlineblock', 'margintop', '5px', 'height', '10px', 'width', '10px', 'border', '1px', 'solid', 'black', 'background', 'red', 'jquery', 'document', 'ready', 'function', 'tooltip', 'hide', '#hover', 'hover', 'function', 'find', 'tooltip', 'fadein', 'function', 'find', 'tooltip', 'delay', '1000', 'fadeout', 'demo', 'http', 'jsfiddlenet', '8gc3d', '2904'], tags=['Train_6']),\n",
              " TaggedDocument(words=['ajax', 'calender', 'working', 'ie', 'using', 'ajax', 'calender', 'readonly', 'textbox', 'control', 'select', 'date', 'click', 'date', 'calender', 'picks', 'date', 'attach', 'txtfromdate', 'working', 'correctly', 'ff', 'chrome', 'ie', 'code', 'asptextbox', 'id', 'txtfromdate', 'text', 'date', 'runat', 'server', 'onfocus', 'javascriptthisvalue', 'onblur', 'javascript', 'thisvalue', 'thisvalue', 'date', 'asptextbox', 'ajaxcalendarextender', 'id', 'txtcalendecontrolextenderfromdate', 'runat', 'server', 'format', 'ddmmmyyyy', 'targetcontrolid', 'txtfromdate', 'ajaxcalendarextender'], tags=['Train_7']),\n",
              " TaggedDocument(words=['c++', 'random', 'number', 'generator', 'hung', 'whenever', 'attempt', 'run', 'code', 'program', 'gets', 'hung', 'print', 'value', 'r2eff', 'sure', 'help', 'would', 'appreciated', 'sample', 'value', 'r2efftemp', 'would', '10', 'stddev', 'would', '05', '5', 'unsigned', 'seed', 'stdchronosystem_clocknow', 'time_since_epoch', 'count', 'stdmt19937', 'generator', 'seed', 'normal_distributiondouble', 'rand', 'r2efftemp', 'r2efftempstddev', 'r2efftemp', 'rand', 'generator', 'coutr2efftempendl', 'thank'], tags=['Train_8']),\n",
              " TaggedDocument(words=['bit', 'vector', 'looked', 'online', 'good', 'seem', 'find', 'good', 'example', 'bit', 'vector', 'actually', 'assignment', 'college', 'add', 'remove', 'union', '2', 'vectors', 'intersection', 'struggling', 'comprehend', 'actual', 'bit', 'vector', 'using', 'c', 'write', 'could', 'someone', 'please', 'help', 'would', 'massive', 'help'], tags=['Train_9'])]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvY-IsHNxoBQ"
      },
      "source": [
        "Gensim allows us to build a model very easily. We can vary the parameters to fit your data: \n",
        "\n",
        "*    `dm=0` , distributed bag of words (DBOW) is used.\n",
        "*    `vector_size=300` , 300 vector dimensional feature vectors.\n",
        "*    `negative=5` , specifies how many “noise words” should be drawn.\n",
        "*    `min_count=1`, ignores all words with total frequency lower than this.\n",
        "*    `alpha=0.065` , the initial learning rate.\n",
        "\n",
        "We initialize the model and train for 30 epochs. (Those of you on slower computers may want to train for less epochs). Be sure to set your runtime to GPU hardware acceleration! Maybe test with a lower amount of epochs first to see how high you can go during class time!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UoqpKnQIpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb6c4e7-1f68-42ff-f795-bd8e1984fe12"
      },
      "source": [
        "model_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, \n",
        "                     min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 2572482.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSDy4huyQIpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd697a6-0e3b-42c7-a5e7-f2fb1be87b59"
      },
      "source": [
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), \n",
        "                     total_examples=len(all_data), \n",
        "                     epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 1762053.48it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2197236.10it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2476780.54it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2123671.35it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2283795.16it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3077654.14it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2166115.71it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2268816.31it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2369529.41it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2437840.16it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2252096.22it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2460976.63it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2401067.06it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2200290.62it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2297178.85it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2361923.64it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2218093.55it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2487946.14it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2157647.03it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2175863.88it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2436459.43it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2330751.58it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2887744.16it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2770117.39it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2442773.98it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2290561.27it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2736679.88it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2202514.80it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2932413.26it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2262147.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNp4a3ouxoB5"
      },
      "source": [
        "Now let's define a function to get the vector of a particular word from this trained  model, so that we can feed them into the logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6cGBFuRQIpT"
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgOEdSpS7nTW"
      },
      "source": [
        "We can use this function to create a vectorised training and test set with 1 entry per document for the input in classification models such as logistic regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57wLrRE3QIpX"
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJE2ZeuOxoCD"
      },
      "source": [
        "We can now feed these vectors to the classifier again: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoLo0XkeQIpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22da4755-be52-40c5-9041-8497d31d6047"
      },
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(train_vectors_dbow, y_train)\n",
        "\n",
        "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
        "y_pred = logreg.predict(test_vectors_dbow)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.7993333333333333\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.67      0.66      0.67       589\n",
            "      android       0.88      0.90      0.89       661\n",
            "    angularjs       0.95      0.95      0.95       606\n",
            "      asp.net       0.77      0.76      0.77       613\n",
            "            c       0.84      0.88      0.86       601\n",
            "           c#       0.70      0.71      0.70       585\n",
            "          c++       0.87      0.81      0.84       621\n",
            "          css       0.82      0.83      0.82       587\n",
            "         html       0.69      0.67      0.68       560\n",
            "          ios       0.66      0.68      0.67       611\n",
            "       iphone       0.63      0.65      0.64       593\n",
            "         java       0.80      0.84      0.82       581\n",
            "   javascript       0.81      0.79      0.80       608\n",
            "       jquery       0.84      0.85      0.85       593\n",
            "        mysql       0.83      0.82      0.83       592\n",
            "  objective-c       0.70      0.63      0.66       597\n",
            "          php       0.84      0.84      0.84       604\n",
            "       python       0.91      0.93      0.92       610\n",
            "ruby-on-rails       0.94      0.94      0.94       595\n",
            "          sql       0.80      0.83      0.81       593\n",
            "\n",
            "     accuracy                           0.80     12000\n",
            "    macro avg       0.80      0.80      0.80     12000\n",
            " weighted avg       0.80      0.80      0.80     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqmumA0-xoCV"
      },
      "source": [
        "We get 80%, that is the best result so far! Remember, we can actually use any classifier with this method! So up to you to make your project as efficient as possible :)\n",
        "    \n",
        "Try using a different classifiers, e.g. Decision tree or SVM. Does that influence the results? \n",
        "\n",
        "New methods are coming out every day in the field of data science. Just at the end of August 2019, the first implementation of BERT for document classfication was published: DocBERT: https://arxiv.org/abs/1904.08398\n",
        "\n",
        "These embeddings can similarly be loaded. There are also specialised pretrainend embeddings for say, financial data, e.g. FinBERT. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvthUI_yxoCg"
      },
      "source": [
        "## References\n",
        "\n",
        "* https://radimrehurek.com/gensim/models/word2vec.html\n",
        "* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
        "* https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec\n",
        "* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDmjCi1QHIkV"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Now over to you! \n",
        "\n",
        "Can you develop a doc2vec with SVM classifier for the following dataset? \n",
        "\n",
        "https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset?select=Fake.csv\n",
        "\n",
        "The task is to predict if news is fake or real. \n",
        "\n",
        "As input, use only the text for simplicity (possibly concatenated with title, but not necessary). \n",
        "\n",
        "Good luck! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QyRLoutHY3g"
      },
      "source": [
        "## solution\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.models import doc2vec\n",
        "from sklearn import utils\n",
        "from sklearn.svm import SVC\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import nltk\n",
        "import lxml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbroVPNL83zW"
      },
      "source": [
        "df_fake = pd.read_csv('https://raw.githubusercontent.com/Pillowkoh/50.038-Lab-7/main/Fake.csv')\n",
        "df_true = pd.read_csv('https://raw.githubusercontent.com/Pillowkoh/50.038-Lab-7/main/True.csv')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiDPp-Uj9z--",
        "outputId": "e9430b89-a7c2-4d52-a0b1-38e52de593ed"
      },
      "source": [
        "df_true.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21417, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r5xtX6r-Cz2",
        "outputId": "23e2e1b8-2d72-4782-ca73-abba2c80c74c"
      },
      "source": [
        "df_true.drop_duplicates().shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21211, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQvWHHyzBnG-"
      },
      "source": [
        "df_fake = df_fake.drop_duplicates()\n",
        "df_true = df_true.drop_duplicates()\n",
        "df_fake['tags'] = 'Fake'\n",
        "df_true['tags'] = 'True'\n",
        "df_total = pd.concat([df_fake,df_true])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "y3cAllzRCgqz",
        "outputId": "7908e0f4-cd21-4415-d051-1c027dbcab75"
      },
      "source": [
        "df_total.head(10)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Racist Alabama Cops Brutalize Black Boy While...</td>\n",
              "      <td>The number of cases of cops brutalizing and ki...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Fresh Off The Golf Course, Trump Lashes Out A...</td>\n",
              "      <td>Donald Trump spent a good portion of his day a...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 23, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Trump Said Some INSANELY Racist Stuff Inside ...</td>\n",
              "      <td>In the wake of yet another court decision that...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 23, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Former CIA Director Slams Trump Over UN Bully...</td>\n",
              "      <td>Many people have raised the alarm regarding th...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 22, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>WATCH: Brand-New Pro-Trump Ad Features So Muc...</td>\n",
              "      <td>Just when you might have thought we d get a br...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 21, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...  tags\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...  Fake\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...  Fake\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...  Fake\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...  Fake\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...  Fake\n",
              "5   Racist Alabama Cops Brutalize Black Boy While...  ...  Fake\n",
              "6   Fresh Off The Golf Course, Trump Lashes Out A...  ...  Fake\n",
              "7   Trump Said Some INSANELY Racist Stuff Inside ...  ...  Fake\n",
              "8   Former CIA Director Slams Trump Over UN Bully...  ...  Fake\n",
              "9   WATCH: Brand-New Pro-Trump Ad Features So Muc...  ...  Fake\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "J7R4Zq5sCmE0",
        "outputId": "61f245f7-0ab3-4232-a418-a8a2e5878c51"
      },
      "source": [
        "df_total.tail(10)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21406</th>\n",
              "      <td>U.S., North Korea clash at U.N. forum over nuc...</td>\n",
              "      <td>GENEVA (Reuters) - North Korea and the United ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21407</th>\n",
              "      <td>Mata Pires, owner of embattled Brazil builder ...</td>\n",
              "      <td>SAO PAULO (Reuters) - Cesar Mata Pires, the ow...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21409</th>\n",
              "      <td>U.S., North Korea clash at U.N. arms forum on ...</td>\n",
              "      <td>GENEVA (Reuters) - North Korea and the United ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21410</th>\n",
              "      <td>Headless torso could belong to submarine journ...</td>\n",
              "      <td>COPENHAGEN (Reuters) - Danish police said on T...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21411</th>\n",
              "      <td>North Korea shipments to Syria chemical arms a...</td>\n",
              "      <td>UNITED NATIONS (Reuters) - Two North Korean sh...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 21, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21412</th>\n",
              "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
              "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21413</th>\n",
              "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
              "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21414</th>\n",
              "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
              "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21415</th>\n",
              "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
              "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21416</th>\n",
              "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
              "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 22, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ...  tags\n",
              "21406  U.S., North Korea clash at U.N. forum over nuc...  ...  True\n",
              "21407  Mata Pires, owner of embattled Brazil builder ...  ...  True\n",
              "21409  U.S., North Korea clash at U.N. arms forum on ...  ...  True\n",
              "21410  Headless torso could belong to submarine journ...  ...  True\n",
              "21411  North Korea shipments to Syria chemical arms a...  ...  True\n",
              "21412  'Fully committed' NATO backs new U.S. approach...  ...  True\n",
              "21413  LexisNexis withdrew two products from Chinese ...  ...  True\n",
              "21414  Minsk cultural hub becomes haven from authorities  ...  True\n",
              "21415  Vatican upbeat on possibility of Pope Francis ...  ...  True\n",
              "21416  Indonesia to buy $1.14 billion worth of Russia...  ...  True\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7VIfO4-6aOK"
      },
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZVp_dGu66uQ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_total.text, df_total.tags, random_state=0, \n",
        "                                                    test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UrGyltADPbZ",
        "outputId": "06ddb71e-dec2-465c-85fd-e4e5f6e8dab8"
      },
      "source": [
        "all_data[:10]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['ADDIS', 'ABABA', '(Reuters)', '-', 'Ethnic', 'clashes', 'killed', '11', 'people', 'this', 'week', 'in', 'Ethiopia', 's', 'Oromiya', 'region,', 'a', 'regional', 'government', 'official', 'said', 'on', 'Sunday,', 'the', 'latest', 'unrest', 'in', 'a', 'province', 'that', 'was', 'wracked', 'by', 'violence', 'in', '2015', 'and', '2016.', 'Nearly', '700', 'people', 'died', 'last', 'year', 'during', 'one', 'period', 'of', 'the', 'violence', 'in', 'Ethiopia', 's', 'largest', 'region', 'and', 'other', 'areas,', 'according', 'to', 'a', 'parliament-mandated', 'investigation.', 'The', 'unrest', 'forced', 'the', 'government', 'to', 'impose', 'a', 'nine-month', 'state', 'of', 'emergency', 'that', 'was', 'finally', 'lifted', 'in', 'August.', 'Sporadic', 'protests', 'have', 'taken', 'place', 'since', 'then.', 'Violence', 'broke', 'out', 'this', 'week', 'in', 'two', 'districts', 'in', 'the', 'province', 's', 'west', 'after', 'protests', 'led', 'to', 'clashes', 'between', 'ethnic', 'Oromos', 'and', 'Amharas,', 'the', 'spokesman', 'for', 'the', 'region', 's', 'administration', 'said', 'on', 'Sunday.', 'Eight', 'Oromos', 'and', 'three', 'Amharas', 'died,', 'spokesman', 'Addisu', 'Arega', 'Kitessa', 'said', 'in', 'a', 'statement.', 'The', 'previous', 'unrest', 'was', 'provoked', 'by', 'a', 'development', 'scheme', 'for', 'the', 'capital,', 'Addis', 'Ababa,', 'that', 'dissidents', 'said', 'amounted', 'to', 'land', 'grabs.', 'Broader', 'anti-government', 'demonstrations', 'followed,', 'over', 'politics', 'and', 'human', 'rights', 'abuses.', 'The', 'violence', 'included', 'attacks', 'on', 'businesses,', 'many', 'of', 'them', 'foreign-owned,', 'including', 'farms', 'growing', 'flowers', 'for', 'export.', 'Separately,', 'clashes', 'along', 'the', 'border', 'between', 'the', 'country', 's', 'Oromiya', 'and', 'Somali', 'regions', 'last', 'month', 'also', 'displaced', 'hundreds', 'of', 'thousands', 'of', 'people.', 'The', 'area', 'has', 'been', 'plagued', 'by', 'sporadic', 'violence', 'for', 'decades.', 'A', 'referendum', 'held', 'in', '2004', 'to', 'determine', 'the', 'status', 'of', 'disputed', 'settlements', 'failed', 'to', 'ease', 'tensions.', 'Those', 'clashes', 'have', 'fueled', 'fears', 'about', 'security', 'in', 'Ethiopia,', 'the', 'region', 's', 'biggest', 'economy', 'and', 'a', 'staunch', 'Western', 'ally.'], tags=['Train_0']),\n",
              " TaggedDocument(words=['MOSCOW', '(Reuters)', '-', 'Six', 'Russian', 'long-range', 'bombers', 'struck', 'Islamic', 'State', 'targets', 'near', 'the', 'town', 'of', 'Albu', 'Kamal', 'in', 'Syria', 's', 'Deir', 'al-Zor', 'Province', 'on', 'Wednesday,', 'the', 'Russian', 'Defence', 'Ministry', 'said', 'in', 'a', 'statement.', 'The', 'TU-22M3', 'bombers', 'took', 'off', 'from', 'bases', 'in', 'Russia', 'and', 'overflew', 'Iran', 'and', 'Iraq', 'before', 'launching', 'the', 'strike,', 'it', 'said.', 'The', 'ministry', 'said', 'the', 'planes', 'had', 'bombed', 'Islamic', 'State', 'supply', 'depots,', 'militants,', 'and', 'armored', 'vehicles', 'and', 'that', 'satellite', 'and', 'drone', 'surveillance', 'had', 'confirmed', 'that', 'all', 'of', 'the', 'designated', 'targets', 'had', 'been', 'destroyed.', 'It', 'said', 'Sukhoi-30SM', 'fighter', 'jets,', 'based', 'at', 'the', 'Hmeymim', 'air', 'base', 'in', 'Syria', 'used', 'by', 'Russian', 'forces,', 'had', 'escorted', 'the', 'bombers', 'while', 'they', 'were', 'in', 'Syrian', 'air', 'space', 'and', 'that', 'all', 'the', 'bombers', 'had', 'safely', 'returned', 'to', 'their', 'bases.', 'Russia', 'on', 'Tuesday', 'accused', 'the', 'United', 'States', 'of', 'providing', 'de-facto', 'cover', 'for', 'Islamic', 'State', 'units', 'in', 'Syria', 'and', 'of', 'only', 'pretending', 'to', 'fight', 'terrorism', 'in', 'the', 'Middle', 'East.', 'Specifically,', 'the', 'Russian', 'Defence', 'Ministry', 'said', 'the', 'U.S.', 'air', 'force', 'had', 'tried', 'to', 'hinder', 'Russian', 'strikes', 'on', 'Islamic', 'State', 'militants', 'around', 'Albu', 'Kamal.', 'Asked', 'about', 'the', 'Russian', 'allegations,', 'Colonel', 'Ryan', 'Dillon,', 'a', 'spokesman', 'for', 'the', 'U.S.-led', 'coalition', 'fighting', 'Islamic', 'State,', 'said:', 'The', 'Russian', 'ministry', 'of', 'defense', 'statements', 'are', 'about', 'as', 'accurate', 'as', 'their', 'air', 'campaign', 'and', 'I', 'think', 'that', 'is', 'a', 'reason', 'for', 'them', 'to', 'start,', 'you', 'know,', 'coming', 'out', 'with', 'their', 'latest', 'barrage', 'of', 'lies.'], tags=['Train_1']),\n",
              " TaggedDocument(words=['The', 'Army', 'can', 't', 'be', 'bothered', 'with', 'defending', 'or', 'protecting', 'war', 'heroes,', 'they', 're', 'too', 'busy', 'with', 'other', 'more', 'pressing', 'issues', 'like', 'ensuring', 'gays', 'are', 'free', 'to', 'come', 'out', 'of', 'the', 'closet', 'and', 'removing', 'any', 'trace', 'of', 'Christianity', 'from', 'our', 'military', 'bases.A', 'Green', 'Beret', 'war', 'hero', 'under', 'investigation', 'for', 'unauthorized', 'communications', 'with', 'Congress', 'will', 'testify', 'on', 'Thursday', 'that', 'the', 'Army', 'is', 'out', 'to', 'court-martial', 'him', 'on', 'criminal', 'charges.Meanwhile,', 'Rep.', 'Duncan', 'Hunter,', 'one', 'of', 'the', 'lawmakers', 'with', 'whom', 'the', 'soldier', 'spoke', 'about', 'Obama', 'administration', 'hostage', 'rescue', 'policies,', 'has', 'asked', 'the', 'Pentagon', 'inspector', 'general', 'to', 'investigate', 'whether', 'the', 'Army', 'is', 'springing', 'allegations', 'against', 'personnel', 'such', 'as', 'the', 'Green', 'Beret', 'as', 'pure', 'retaliation.Lt.', 'Col.', 'Jason', 'Amerine,', 'the', 'Special', 'Forces', 'soldier,', 'is', 'due', 'to', 'testify', 'before', 'the', 'Senate', 'Committee', 'on', 'Homeland', 'Security', 'and', 'Governmental', 'Affairs', 'about', 'what', 'he', 'considers', 'reprisals', 'against', 'him', 'as', 'a', 'whistleblower.', 'After', 'I', 'made', 'protected', 'disclosures', 'to', 'Congress,', 'the', 'Army', 'suspended', 'my', 'clearance,', 'removed', 'me', 'from', 'my', 'job,', 'launched', 'a', 'criminal', 'investigation', 'and', 'deleted', 'my', 'retirement', 'orders', 'with', 'a', 'view', 'to', 'court', 'martial', 'me', 'after', 'I', 'exercised', 'that', 'Constitutional', 'right,', 'says', 'Col.', 'Amerine,', 'according', 'to', 'partial', 'remarks', 'provided', 'to', 'The', 'Washington', 'Times', 'by', 'Mr.', 'Hunter', 's', 'office.', 'For', 'nearly', 'five', 'months,', 'I', 'have', 'received', 'no', 'relief', 'from', 'the', 'military', 'and', 'there', 'has', 'been', 'no', 'transparency', 'in', 'their', 'investigation', 'of', 'me.', 'My', 'pay', 'was', 'even', 'stopped', 'briefly', 'after', 'the', 'Army', 'deleted', 'my', 'retirement', 'orders.', 'Mr.', 'Hunter,', 'California', 'Republican', 'and', 'member', 'of', 'the', 'House', 'Armed', 'Services', 'Committee,', 'wrote', 'to', 'Jon', 'T.', 'Rymer,', 'the', 'inspector', 'general,', 'about', 'three', 'soldiers', 'against', 'whom', 'he', 'believed', 'Army', 'Secretary', 'John', 'McHugh', 'retaliated.', 'He', 'asked', 'for', 'an', 'IG', 'investigation', 'into', 'Mr.', 'McHugh', 's', 'use', 'of', 'the', 'Criminal', 'Investigation', 'Command.', 'Specifically,', 'my', 'concern', 'is', 'that', 'the', 'Army', 'under', 'the', 'leadership', 'of', 'Secretary', 'of', 'the', 'Army', 'has', 'used', 'CID', 'for', 'the', 'purpose', 'of', 'influencing', 'actions/outcomes', 'and', 'retaliating', 'against', 'soldiers,', 'he', 'wrote.He', 'cited', 'three', 'soldiers:', 'Col.', 'Amerine.', 'The', 'officer', 'worked', 'in', 'a', 'small', 'Army', 'unit', 'at', 'the', 'Pentagon', 'and', 'he', 'devoted', 'himself', 'to', 'developing', 'policies', 'to', 'gain', 'the', 'release', 'of', 'Americans', 'held', 'by', 'Islamic', 'extremists.', 'He', 'came', 'to', 'believe', 'the', 'administration', 's', 'hostage', 'policies', 'were', 'in', 'disarray', 'and', 'told', 'Mr.', 'Hunter.', 'The', 'congressman', 'proposed', 'legislation', 'to', 'create', 'a', 'hostage', 'coordinator', 'to', 'work', 'with', 'the', 'various', 'agencies', 'involved,', 'such', 'as', 'the', 'FBI,', 'Pentagon', 'and', 'State', 'Department.', 'Maj.', 'Matt', 'Golsteyn.', 'Also', 'a', 'Green', 'Beret,', 'Maj.', 'Golsteyn', 'saw', 'his', 'valor', 'awards', 'and', 'Special', 'Forces', 'tab', 'stripped', 'by', 'Secretary', 'McHugh.', 'The', 'officer', 'was', 'accused', 'of', 'killing', 'a', 'Taliban', 'bomb-maker,', 'but', 'was', 'never', 'officially', 'charged.', 'He', 'faces', 'a', 'board', 'of', 'inquiry', 'hearing.', 'Sgt.', 'First', 'Class', 'Earl', 'Plumlee.', 'Sgt.', 'Plumlee', 'was', 'nominated', 'for', 'the', 'Medal', 'of', 'Honor,', 'the', 'nation', 's', 'highest', 'military', 'award,', 'for', 'repelling', 'an', 'attack', 'on', 'a', 'base', 'in', 'Afghanistan', 'and', 'saving', 'lives.', 'The', 'MOH', 'was', 'endorsed', 'by', 'top', 'commanders,', 'including', 'Marine', 'Gen.', 'Joseph', 'Dunford,', 'the', 'next', 'chairman', 'of', 'the', 'Joint', 'Chiefs', 'of', 'Staff.', 'But', 'Mr.', 'McHugh', 'downgraded', 'the', 'award', 'to', 'a', 'Silver', 'Star.', 'Mr.', 'Hunter', 'said', 'it', 'was', 'based', 'on', 'a', 'questionable', 'CID', 'investigation.', 'Col.', 'Amerine', 'already', 'has', 'filed', 'a', 'whistleblower', 'retaliation', 'complaint', 'with', 'the', 'IG.', 'The', 'Army', 'probe', 'began', 'after', 'the', 'FBI', 'complained', 'that', 'Col.', 'Amerine', 'was', 'providing', 'information', 'to', 'Congress.', 'I', 'have', 'personally', 'met', 'with', 'the', 'Federal', 'Bureau', 'of', 'Investigation', 'on', 'the', 'developments', 'leading', 'up', 'to', 'the', 'Army', 'investigation,', 'Mr.', 'Hunter', 'told', 'Mr.', 'Rymer.', 'An', 'investigation,', 'I', 'firmly', 'believe,', 'was', 'not', 'warranted,', 'but', 'continues', 'to', 'be', 'unjustifiably', 'delayed.', 'Col.', 'Amerine', 'holds', 'a', 'special', 'place', 'in', 'the', 'history', 'of', 'the', 'Afghanistan', 'war.', 'He', 'led', 'a', 'joint', 'Green', 'Beret-Afghan', 'team', 'in', 'the', '2001', 'invasion,', 'and', 'fought', 'along', 'side', 'Hamid', 'Karzai,', 'the', 'future', 'president.An', 'Army', 'spokesman', 'has', 'said,', 'As', 'a', 'matter', 'of', 'policy,', 'we', 'do', 'not', 'confirm', 'the', 'names', 'of', 'individuals', 'who', 'may', 'or', 'may', 'not', 'be', 'under', 'investigation', 'to', 'protect', 'the', 'integrity', 'of', 'a', 'possible', 'ongoing', 'investigation,', 'as', 'well', 'as', 'the', 'privacy', 'rights', 'of', 'all', 'involved.', 'However,', 'I', 'note', 'that', 'both', 'the', 'law', 'and', 'Army', 'policy', 'would', 'prohibit', 'initiating', 'an', 'investigation', 'based', 'solely', 'on', 'a', 'Soldier', 's', 'protected', 'communications', 'with.Via:', 'Washington', 'Times'], tags=['Train_2']),\n",
              " TaggedDocument(words=['Another', 'victory', 'for', 'one', 'of', 'the', 'most', 'bad', 'ass', 'Sheriff', 's', 'in', 'America', 'A', 'federal', 'judge', 'has', 'upheld', 'part', 'of', 'Arizona', 's', 'contentious', 'immigration', 'law,', 'rejecting', 'claims', 'that', 'the', 'so-called', 'show', 'your', 'papers', 'section', 'of', 'the', 'law', 'discriminated', 'against', 'Hispanics.The', 'ruling', 'by', 'U.S.', 'District', 'Judge', 'Susan', 'Bolton', 'on', 'Friday', 'was', 'on', 'the', 'last', 'of', 'seven', 'challenges', 'to', 'the', '2010', 'law.', 'The', 'section', 'being', 'upheld', 'allows', 'police', 'in', 'Arizona', 'to', 'check', 'the', 'immigration', 'status', 'of', 'anyone', 'they', 'stop.Bolton', 'ruled', 'that', 'immigration', 'rights', 'activists', 'failed', 'to', 'show', 'that', 'police', 'would', 'enforce', 'the', 'law', 'differently', 'for', 'Hispanics', 'than', 'other', 'people.', 'The', 'section', 'is', 'sometimes', 'called', 'the', 'show', 'your', 'papers', 'provision.The', 'judge', 'also', 'upheld', 'a', 'section', 'that', 'let', 'police', 'check', 'to', 'see', 'if', 'a', 'detainee', 'is', 'in', 'the', 'United', 'States', 'illegally.', 'Bolton', 'voided', 'any', 'laws', 'targeting', 'day', 'laborers.Karen', 'Tumlin,', 'the', 'legal', 'director', 'of', 'the', 'National', 'Immigration', 'Law', 'Center,', 'one', 'of', 'the', 'parties', 'to', 'the', 'suit,', 'said', 'the', 'group', 'was', 'evaluating', 'its', 'options.', 'We', 'will', 'continue', 'working', 'on', 'behalf', 'of', 'our', 'courageous', 'plaintiffs', 'to', 'show', 'that', 'Arizona', 'can', 'do', 'better', 'than', 'this', 'disgraceful', 'law,', 'she', 'said', 'in', 'a', 'statement.Bolton', 's', 'ruling', 'came', 'two', 'days', 'after', 'a', 'federal', 'judge', 'approved', 'a', 'deal', 'between', 'the', 'U.S.', 'Department', 'of', 'Justice', 'and', 'Arizona', 's', 'Maricopa', 'County', 'to', 'resolve', 'accusations', 'of', 'civil', 'rights', 'abuses', 'and', 'dismissed', 'the', 'department', 's', 'lawsuit', 'against', 'Sheriff', 'Joe', 'Arpaio', 'and', 'his', 'deputies.', 'Via:', 'Reuters'], tags=['Train_3']),\n",
              " TaggedDocument(words=['PELOSI', 'CALLED', 'ABORTION', 'A', 'MANHOOD', 'THING', ':House', 'Minority', 'Leader', 'Nancy', 'Pelosi', 'had', 'this', 'odd', 'thing', 'to', 'say', 'about', 'defunding', 'Planned', 'Parenthood:', 'What', 'are', 'they', 'doing', 'in', 'this', 'bill?', 'Overturning', 'the', 'Affordable', 'Care', 'Act,', 'undermining', 'the', 'health,', 'security,', 'and', 'financial', 'stability', 'of', 'America', 's', 'working', 'families,', 'and', 'defunding', 'Planned', 'Parenthood', 'That', 's', 'their', 'that', 's', 'their', 'manhood', 'thing', 'Manhood', 'thing???'], tags=['Train_4']),\n",
              " TaggedDocument(words=['BRATISLAVA', '(Reuters)', '-', 'Slovakia', 'will', 'postpone', 'a', 'planned', 'fighter', 'jet', 'purchase', 'pending', 'a', 'broader', 'upgrade', 'of', 'its', 'armed', 'forces,', 'the', 'defense', 'minister', 'said', 'on', 'Wednesday,', 'following', 'warnings', 'that', 'such', 'a', 'move', 'could', 'prolong', 'its', 'dependence', 'on', 'Russia.', 'The', 'central', 'European', 'NATO', 'member', 'state', 'has', 'been', 'in', 'talks', 'with', 'several', 'firms', 'about', 'replacing', 'its', 'aging', 'Russian-made', 'MiG-29s.', 'Peter', 'Gajdos', 'mentioned', 'U.S.', 'F-16', 'fighter', 'jets', 'from', 'Lockheed', 'Martin', 'and', 'Gripen', 'jets', 'made', 'by', 'Sweden', 's', 'Saab', 'as', 'options.', 'Neighboring', 'Hungary', 'and', 'the', 'Czech', 'Republic', 'already', 'operate', 'the', 'latter.', 'Slovakia', 'was', 'expected', 'to', 'make', 'decision', 'by', 'the', 'end', 'of', 'this', 'month,', 'but', 'Gajdos', 'said', 'this', 'would', 'be', 'delayed.', 'We', 'need', 'to', 'modernize', 'the', 'ground', 'forces', 'which', 'are', 'the', 'backbone', 'of', 'our', 'military.', 'While', 'we', 'have', 'been', 'replacing', 'some', 'parts', 'of', 'the', 'air', 'force,', 'the', 'ground', 'forces', 'have', 'not', 'been', 'renewed', 'for', 'decades,', 'he', 'said.', 'Slovakia', 'is', 'due', 'to', 'spend', 'about', '6.5', 'billion', 'euros', '($7.80', 'billion)', 'by', '2030', 'to', 'modernize', 'its', 'army,', 'the', 'defense', 'ministry', 'said', 'in', 'a', 'strategy', 'document,', 'released', 'last', 'week', 'and', 'to', 'be', 'debated', 'in', 'the', 'first', 'half', 'of', '2018.', 'Its', 'spending', 'on', 'defense', 'is', 'due', 'to', 'rise', 'from', '1.1', 'percent', 'of', 'GDP', 'to', '1.6', 'percent', 'in', '2020', 'and', '2.0', 'percent', 'by', '2024.', 'Prime', 'Minister', 'Robert', 'Fico', 'said', 'last', 'month', 'Slovakia', 'should', 'pick', 'European', 'solutions', 'for', 'the', 'army', 'if', 'it', 'wanted', 'to', 'be', 'part', 'of', 'a', 'more', 'deeply', 'integrated', 'core', 'European', 'Union.', 'Gajdos,', 'from', 'the', 'nationalist,', 'eurosceptic', 'SNS', 'party', 'that', 'forms', 'part', 'of', 'the', 'governing', 'coalition,', 'has', 'faced', 'criticism', 'from', 'both', 'its', 'partners', 'Fico', 's', 'leftist', 'Smer', 'and', 'the', 'centrist', 'Most', 'party', 'that', 'delays', 'in', 'closing', 'the', 'fighter', 'deal', 'could', 'prolong', 'Slovakia', 's', 'dependence', 'on', 'Russia.', 'If', 'the', 'finance', 'ministry', 'and', 'government', 'earmark', 'money', 'for', 'all', 'projects,', 'we', 'are', 'ready', 'to', 'finalize', 'the', 'ongoing', 'negotiations', 'and', 'close', 'the', 'deal', 'on', 'new', 'jets,', 'Gajdos', 'said.', 'In', '2015,', 'Slovakia', 'made', 'a', 'deal', 'to', 'buy', 'nine', 'U.S.-made', 'Black', 'Hawk', 'helicopters', 'to', 'replace', 'its', 'Russian', 'Mi-17', 'fleet,', 'and', 'signed', 'with', 'Italy', 's', 'Alenia', 'Aermacchi', 'for', 'two', 'Spartan', 'C-27J', 'transport', 'aircraft', 'to', 'replace', 'Russian', 'Antonovs', 'the', 'year', 'before.', 'The', 'government', 'already', 'approved', 'a', 'plan', 'to', 'spend', '1.2', 'billion', 'euros', '($1.44', 'billion)', 'to', 'replace', 'its', 'outdated', 'armored', 'personnel', 'carriers', 'in', 'May.', 'Slovakia', 'has', 'a', 'maintenance', 'contract', 'with', 'Russia', 'for', 'its', '12', 'MiG-29s', 'until', 'autumn', '2019.', 'If', 'it', 'does', 'not', 'order', 'new', 'jets', 'soon', 'it', 'would', 'need', 'to', 'prolong', 'that,', 'as', 'they', 'would', 'typically', 'take', '18-24', 'months', 'to', 'deliver.', 'Prolongation', 'of', 'the', 'Mig-29s', 'contract', 'is', 'the', 'plan', 'B,', 'the', 'plan', 'A', 'is', 'the', 'purchase', 'of', 'new', 'jets.', 'I', 'won', 't', 'accept', 'any', 'other', 'than', 'a', 'pro-European', 'and', 'pro-Atlantic', 'solution', '(for', 'their', 'replacement),', 'Gajdos', 'said.'], tags=['Train_5']),\n",
              " TaggedDocument(words=['Donald', 'Trump', 'made', 'this', 'way', 'too', 'easy', 'for', 'George', 'Takei.On', 'Saturday', 'morning,', 'Trump', 'lashed', 'out', 'at', 'the', 'cast', 'of', 'Hamilton', 'because', 'the', 'audience', 'booed', 'Mike', 'Pence', 'before', 'the', 'play', 'on', 'Friday', 'night,', 'and', 'the', 'cast', 'delivered', 'a', 'short', 'message', 'afterwards', 'declaring', 'their', 'hope', 'that', 'he', 'learned', 'something', 'from', 'watching.First,', 'Trump', 'whined', 'about', 'so-called', 'harassment,', 'and', 'complained', 'about', 'how', 'this', 'should', 'not', 'happen.', 'Our', 'wonderful', 'future', 'V.P.', 'Mike', 'Pence', 'was', 'harassed', 'last', 'night', 'at', 'the', 'theater', 'by', 'the', 'cast', 'of', 'Hamilton,', 'cameras', 'blazing.This', 'should', 'not', 'happen!', 'Donald', 'J.', 'Trump', '(@realDonaldTrump)', 'November', '19,', '2016Then', 'he', 'demanded', 'an', 'apology', 'from', 'the', 'cast', 'for', 'somehow', 'violating', 'Pence', 's', 'safe', 'space.The', 'Theater', 'must', 'always', 'be', 'a', 'safe', 'and', 'special', 'place.The', 'cast', 'of', 'Hamilton', 'was', 'very', 'rude', 'last', 'night', 'to', 'a', 'very', 'good', 'man,', 'Mike', 'Pence.', 'Apologize!', 'Donald', 'J.', 'Trump', '(@realDonaldTrump)', 'November', '19,', '2016Trump', 'was', 'immediately', 'mocked', 'for', 'his', 'posts,', 'but', 'the', 'award', 'for', 'best', 'response', 'goes', 'to', 'actor', 'George', 'Takei,', 'who', 'masterfully', 'turned', 'Trump', 's', 'second', 'Twitter', 'post', 'against', 'him', 'as', 'a', 'message', 'from', 'the', 'nation.AMERICA', 'must', 'always', 'be', 'a', 'safe', 'and', 'special', 'place.', 'The', 'Trump', 'administration', 'has', 'been', 'very', 'cruel', 'to', 'many', 'good', 'people.', 'Apologize!', 'https://t.co/ndavyD3su6', 'George', 'Takei', '(@GeorgeTakei)', 'November', '19,', '2016That', 's', 'right.', 'Trump', 'is', 'being', 'a', 'massive', 'hypocrite', 'as', 'usual,', 'and', 'it', 'came', 'back', 'to', 'bite', 'him', 'on', 'the', 'ass.But', 'Takei', 'wasn', 't', 'done.If', 'Trump', 'gets', 'upset', 'at', 'a', 'NY', 'theater', 'audience', 'booing', 'his', 'VP,', 'imagine', 'what', 'he', 'll', 'feel', 'like', 'on', 'inauguration', 'when', 'millions', 'cry', 'out', 'against', 'him.', 'George', 'Takei', '(@GeorgeTakei)', 'November', '19,', '2016The', 'Internet', 'responds', 'quickly', 'pic.twitter.com/jhtb4BSDIe', 'George', 'Takei', '(@GeorgeTakei)', 'November', '19,', '2016I', 'wonder', 'if', 'Pence', 'went', 'to', 'Hamilton', 'to', 'take', 'our', 'focus', 'off', 'the', 'Trump', 'University', 'fraud', 'settlement.', 'This', 'administration', 'is', 'morally', 'bankrupt.', 'George', 'Takei', '(@GeorgeTakei)', 'November', '19,', '2016For', 'a', 'year', 'and', 'half,', 'Donald', 'Trump', 'has', 'used', 'offensive', 'hate', 'speech', 'and', 'divisive', 'rhetoric.', 'He', 'has', 'insulted', 'just', 'about', 'every', 'group', 'in', 'America,', 'but', 'he', 'expects', 'everyone', 'to', 'respect', 'him', 'and', 'Pence', 'now.They', 'are', 'pathetic', 'and', 'disgraceful', 'and', 'deserve', 'all', 'the', 'ridicule', 'they', 'are', 'getting', 'from', 'the', 'American', 'people.Featured', 'Image:', 'Wikimedia'], tags=['Train_6']),\n",
              " TaggedDocument(words=['TORONTO', '(Reuters)', '-', 'Black', 'people', 'in', 'Canada', 's', 'most', 'populous', 'province', 'spent', 'longer', 'behind', 'bars', 'awaiting', 'trial', 'than', 'white', 'people', 'charged', 'with', 'many', 'of', 'the', 'same', 'categories', 'of', 'crimes', 'in', 'each', 'of', 'the', 'past', 'five', 'years,', 'according', 'to', 'data', 'obtained', 'by', 'Reuters.', 'Between', 'April', '2015', 'and', 'April', '2016,', 'the', 'most', 'recent', 'period', 'in', 'which', 'data', 'is', 'available,', 'black', 'people', 'awaiting', 'trial', 'in', 'Ontario', 'jails', 'were', 'there', 'longer,', 'on', 'average,', 'than', 'white', 'people', 'charged', 'with', 'the', 'same', 'crime', 'in', '11', 'of', '16', 'offense', 'categories', 'Reuters', 'examined.', 'There', 'were', 'approximately', '6,000', 'black', 'people', 'and', 'nearly', '26,000', 'white', 'people', 'remanded', 'to', 'pre-trial', 'detention', 'during', 'the', 'period.', 'The', 'data', 'showed', 'similar', 'patterns', 'in', 'the', 'four', 'prior', 'years.', '(Graphic:', 'Racial', 'disparities', 'in', 'pre-trial', 'detention', '-', 'tmsnrt.rs/2z18vS7)', 'Among', 'the', 'categories', 'examined,', 'black', 'people', 'spent', 'almost', 'twice', 'as', 'long', 'in', 'remand', 'in', '2015-2016', 'for', 'weapons', 'offenses,', 'equivalent', 'to', 'an', 'additional', '38', 'days.', 'They', 'also', 'spent', '46', 'percent', 'longer', 'for', 'serious', 'violent', 'offenses', 'and', '36', 'percent', 'longer', 'on', 'charges', 'of', 'obstructing', 'justice.', 'In', 'three', 'categories,', 'white', 'people', 'awaiting', 'trial', 'were', 'held', 'longer', 'in', 'remand', 'during', 'the', 'same', 'period.', 'Those', 'included', 'drug', 'possession,', 'theft', 'and', 'traffic', 'offenses.', 'In', 'two', 'categories,', 'the', 'difference', 'was', '1', 'percent', 'or', 'less.', 'The', 'data', 'also', 'showed', 'black', 'people', 'arrested', 'and', 'held', 'in', 'custody', 'between', '2011', 'and', '2016', 'were', 'more', 'likely', 'than', 'white', 'people', 'to', 'spend', 'more', 'than', 'a', 'year', 'in', 'pre-trial', 'detention.', 'Reuters', 'obtained', 'the', 'previously', 'unreported', 'data', 'through', 'access-to-information', 'requests', 'from', 'Ontario,', 'which', 'asks', 'inmates', 'to', 'indicate', 'their', 'race', 'when', 'they', 'enter', 'jail.', 'Other', 'provinces', 'do', 'not', 'collect', 'this', 'data', 'or', 'categorize', 'it', 'differently.', 'A', 'spokesman', 'for', 'Ontario', 'Attorney', 'General', 'Yasir', 'Naqvi', 'said', 'the', 'province', 'takes', 'systemic', 'racism', 'seriously', 'and', 'is', 'working', 'to', 'address', 'racial', 'inequities,', 'but', 'declined', 'to', 'comment', 'on', 'the', 'data.', 'The', 'Ontario', 'Crown', 'Attorneys', 'Association,', 'which', 'represents', 'the', 'province', 's', 'prosecutors,', 'and', 'the', 'Association', 'of', 'Justices', 'of', 'the', 'Peace,', 'which', 'represents', 'the', 'people', 'who', 'decide', 'most', 'of', 'Ontario', 's', 'bail', 'cases,', 'declined', 'to', 'comment.', 'More', 'than', 'a', 'dozen', 'defense', 'lawyers', 'as', 'well', 'as', 'prosecutors,', 'criminologists,', 'and', 'a', 'judge', 'interviewed', 'by', 'Reuters', 'said', 'shortcomings', 'in', 'Canada', 's', 'bail', 'system', 'appeared', 'to', 'play', 'a', 'role', 'in', 'the', 'racial', 'disparities', 'shown', 'in', 'the', 'data.', 'Unlike', 'the', 'United', 'States,', 'Canada', 'virtually', 'eliminated', 'cash', 'bail', 'almost', 'half', 'a', 'century', 'ago.', 'Instead,', 'courts', 'often', 'require', 'prisoners', 'awaiting', 'trial', 'to', 'secure', 'a', 'surety,', 'meaning', 'a', 'relative', 'or', 'close', 'friend', 'who', 'can', 'appear', 'in', 'court', 'and', 'subsequently', 'monitor', 'them.', 'A', 'surety', 'needs', 'assets', 'to', 'pledge,', 'a', 'crime-free', 'record', 'and,', 'often,', 'a', 'home', 'where', 'the', 'accused', 'person', 'can', 'live', 'until', 'the', 'case', 'is', 'complete.', 'A', 'surety', 'cannot', 'represent', 'more', 'than', 'one', 'defendant', 'at', 'a', 'time.', 'Current', 'and', 'former', 'prosecutors', 'interviewed', 'for', 'this', 'story', 'said', 'securing', 'a', 'surety', 'can', 'be', 'onerous', 'and', 'the', 'requirement', 'is', 'perhaps', 'relied', 'upon', 'too', 'often;', 'but', 'some', 'said', 'sureties', 'remain', 'the', 'best', 'way', 'to', 'protect', 'the', 'public', 'and', 'ensure', 'defendants', 'show', 'up', 'for', 'trial.', 'Critics', 'of', 'the', 'system', 'say', 'the', 'poor', 'are', 'less', 'likely', 'than', 'middle-class', 'or', 'wealthy', 'people', 'to', 'have', 'connections', 'to', 'provide', 'the', 'assets', 'to', 'pledge', 'or', 'housing', 'to', 'act', 'as', 'a', 'surety.', 'They', 'add', 'that', 'this', 'has', 'an', 'outsized', 'impact', 'on', 'minorities,', 'who', 'are', 'over-represented', 'among', 'Canada', 's', 'poor.', 'Surety', 'is', 'a', 'huge', 'issue', 'in', 'Ontario,', 'said', 'Nicole', 'Myers,', 'a', 'criminologist', 'at', 'Simon', 'Fraser', 'University', 'in', 'British', 'Columbia.', 'If', 'you', 'are', 'from', 'a', 'marginalized', 'community', 'or', 'a', 'criminalized', 'community,', 'it', 'can', 'be', 'very', 'difficult', 'to', 'find', 'a', 'surety', 'the', 'court', 'deems', 'appropriate.', 'The', 'data', 'did', 'not', 'take', 'into', 'account', 'specifics', 'of', 'each', 'case,', 'the', 'person', 's', 'criminal', 'record,', 'the', 'frequency', 'of', 'plea', 'deals,', 'whether', 'the', 'person', 'had', 'a', 'bail', 'hearing', 'and', 'why', 'bail', 'may', 'have', 'been', 'denied.', 'Reuters', 'focused', 'on', 'offenses', 'with', 'the', 'largest', 'pre-trial', 'populations', 'when', 'comparing', 'the', 'average', 'periods', 'in', 'remand,', 'to', 'minimize', 'the', 'impact', 'of', 'outliers.', 'Inmates', 'charged', 'in', 'multiple', 'offense', 'categories', 'were', 'counted', 'in', 'only', 'the', 'more', 'serious', 'one;', 'multiple', 'charges', 'could', 'affect', 'someone', 's', 'chances', 'of', 'getting', 'bail.', 'Studies,', 'including', 'one', 'published', 'last', 'year', 'by', 'the', 'Ottawa', 'police,', 'have', 'found', 'Ontario', 's', 'black', 'communities', 'are', 'more', 'heavily', 'policed', 'than', 'white', 'ones.', 'This', 'makes', 'black', 'people', 'more', 'likely', 'to', 'be', 'caught', 'breaching', 'bail', 'and', 'makes', 'it', 'harder', 'to', 'find', 'a', 'surety', 'without', 'a', 'criminal', 'record', 'who', 'is', 'not', 'serving', 'as', 'surety', 'for', 'someone', 'else,', 'said', 'Chris', 'Sewrattan,', 'a', 'defense', 'lawyer', 'who', 'represents', 'many', 'young', 'black', 'men', 'from', 'eastern', 'Toronto.', 'In', 'a', 'ruling', 'this', 'year,', 'Canada', 's', 'Supreme', 'Court', 'called', 'sureties', 'one', 'of', 'the', 'most', 'onerous', 'forms', 'of', 'release,', 'not', 'to', 'be', 'used', 'unless', 'other', 'options', 'have', 'been', 'considered,', 'such', 'as', 'programs', 'that', 'assign', 'a', 'case', 'worker', 'and', 'require', 'the', 'accused', 'to', 'check', 'in', 'regularly', 'with', 'the', 'courts.', 'The', 'court', 'did', 'not', 'address', 'race', 'in', 'its', 'ruling.', 'At', 'least', 'six', 'provincial', 'governments', 'in', 'Canada,', 'including', 'those', 'of', 'Alberta,', 'British', 'Columbia,', 'and', 'Manitoba,', 'have', 'said', 'they', 'are', 'reviewing', 'bail', 'practices.', 'An', 'earlier', 'Reuters', 'investigation', 'found', 'inmates', 'awaiting', 'trial', 'are', 'more', 'likely', 'to', 'die', 'behind', 'bars', 'than', 'their', 'sentenced', 'counterparts.'], tags=['Train_7']),\n",
              " TaggedDocument(words=['NEW', 'YORK', '(Reuters)', '-', 'The', 'cost', 'to', 'healthcare', 'companies', 'for', 'U.S.', 'regulatory', 'review', 'of', 'their', 'products,', 'including', 'drugs', 'and', 'medical', 'devices,', 'would', 'more', 'than', 'double', 'under', 'the', 'Trump', 'administration’s', 'proposed', '2018', 'budget.', 'For', '2018,', 'the', 'Trump', 'administration', 'has', 'budgeted', 'over', '$2', 'billion', 'in', 'fees', 'to', 'be', 'collected', 'by', 'the', 'U.S.', 'Food', 'and', 'Drug', 'Administration', 'from', 'industry,', 'twice', 'as', 'much', 'as', 'in', '2017,', 'according', 'to', 'budget', 'documents', 'released', 'on', 'Thursday.', 'Citing', 'a', 'constrained', 'budget', 'environment,', 'the', 'proposed', 'budget', 'said', 'industries', 'that', 'benefit', 'from', 'the', 'FDA’s', 'approval', '“can', 'and', 'should', 'pay', 'for', 'their', 'share.”', 'In', 'return,', 'the', 'budget', 'said', 'it', 'also', 'offered', 'measures', 'that', 'would', 'help', 'speed', 'up', 'the', 'approval', 'process', 'for', 'new', 'drugs', 'and', 'other', 'products.', 'The', 'FDA', 'has', 'been', 'criticized', 'by', 'lawmakers', 'for', 'not', 'being', 'quick', 'enough', 'at', 'approving', 'drugs,', 'and', 'President', 'Donald', 'Trump', 'told', 'Congress', 'earlier', 'this', 'year', 'that', 'he', 'aimed', 'to', 'speed', 'up', 'the', 'approval', 'of', 'drugs.', 'The', 'FDA', 'has', 'been', 'charging', 'companies', 'to', 'review', 'their', 'products', 'since', '1992.', 'Most', 'of', 'the', 'user', 'fees', 'collected', 'are', 'for', 'prescription', 'drugs', '-', 'around', '$866', 'million', 'estimated', 'in', '2017', '-', 'and', 'generic', 'drugs', '-', 'around', '$324', 'million,', 'according', 'to', 'the', 'FDA', 'website.', 'The', 'FDA’s', '2017', 'budget', 'was', '$5.1', 'billion,', 'the', 'website', 'said.', 'The', 'budget', 'does', 'not', 'say', 'if', 'the', 'fee', 'increases', 'would', 'be', 'evenly', 'spread', 'or', 'directed', 'in', 'a', 'particular', 'area.', 'The', 'budget', 'did', 'not', 'provide', 'specifics', 'on', 'what', 'measures', 'to', 'speed', 'up', 'approvals', 'might', 'include.'], tags=['Train_8']),\n",
              " TaggedDocument(words=['Greta', 'van', 'Susteren', 'left', 'Fox', 'News', 'abruptly', 'recently,', 'but', 'she', 's', 'having', 'a', 'bit', 'of', 'fun', 'trolling', 'the', 'network.', 'When', 'someone', 'leaves,', 'standard', 'procedure', 'is', 'to', 'redirect', 'their', 'news', 'page', 'back', 'to', 'Fox', 's', 'own', 'website.', 'However,', 'they', 'also', 'redirected', 'her', 'personal', 'page,', 'Greta.com.', 'That', 's', 'not', 'right', 'she', 'owns', 'the', 'domain.', 'So', 'she', 'took', 'matters', 'into', 'her', 'own', 'hands', 'and', 'redirected', 'it', 'somewhere', 'else.She', 'redirected', 'it', 'to', 'an', 'animal', 'rescue', 'site', 'called', 'PetsConnect', 'Rescue.', 'PetsConnect', 'noticed', 'that,', 'and', 'was', 'so', 'grateful', 'for', 'it', 'that', 'they', 'even', 'tweeted', 'a', 'thank-you', 'at', 'her.', 'Since', 'Fox', 'is', 'right-leaning,', 'and', 'also', 'named', 'after', 'an', 'animal', '(unless', 'it', 'was', 'named', 'for', 'men', 's', 'views', 'of', 'women', 'as', 'eye', 'candy),', 'that', 's', 'punking', 'a', 'little', 'hard.', 'Animal', 'rights', 'and', 'welfare', 'do', 'not', 'matter', 'to', 'the', 'right', 'at', 'all.Not', 'only', 'that,', 'but', 'on', 'the', 'chance', 'that', 'Fox', 'stole', 'her', 'domain', 'to', 'needle', 'her,', 'and', 'it', 'wasn', 't', 'an', 'honest', 'mistake,', 'needling', 'them', 'back', 'with', 'a', 'sharp', 'pipe', 'like', 'this', 'seems', 'to', 'be', 'the', 'most', 'reasonable', 'thing', 'to', 'do.Fox', 'News', 'is', 'falling', 'apart', 'at', 'the', 'seams', 'with', 'all', 'of', 'the', 'sexual', 'harassment', 'allegations', 'being', 'thrown', 'at', 'them,', 'and', 'also', 'with', 'Gretchen', 'Carlson', 's', '$20', 'million', 'settlement.', 'The', 'network', 'also', 'publicly', 'apologized', 'to', 'her,', 'and', 'settled', 'with', 'a', 'handful', 'of', 'other', 'women', 'as', 'well.It', 'also', 'recently', 'came', 'out', 'that', 'they', 're', 'guilty', 'of', 'hacking', 'into', 'the', 'personal', 'phones', 'of', 'journalists', 'at', 'other', 'news', 'outlets', 'to', 'find', 'out', 'who', 'they', 'were', 'talking', 'to.', 'In', 'short,', 'Roger', 'Ailes', 'maintained', 'one', 'of', 'the', 'biggest', 'tissues', 'of', 'lies', 'ever:', 'Running', 'his', 'network', 'into', 'the', 'ground', 'and', 'calling', 'it', 'a', 'success.The', 'toilet', 'is', 'too', 'lofty', 'a', 'place', 'to', 'describe', 'the', 'depths', 'to', 'which', 'they', 'have', 'sunk.Van', 'Susteren', 's', 'website', 'is', 'Greta.com.', 'Since', 'it', 's', 'her', 'domain,', 'it', 'will', 'go', 'where', 'she', 'wants', 'it', 'to.', 'That', 's', 'her', 'right,', 'and', 'Fox', 'News', 'can', 'go', 'suck', 'it.Featured', 'image', 'by', 'Nancy', 'Ostertag/Getty', 'Images'], tags=['Train_9'])]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSGAJTdhDai7",
        "outputId": "88c635c4-99dc-48ce-fa7b-2b314ad55757"
      },
      "source": [
        "all_data[-10:]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['The', 'plot', 'thickens', 'Republican', 'donor', 'and', 'operative', 'from', 'Chicago', 's', 'North', 'Shore', 'who', 'said', 'he', 'had', 'tried', 'to', 'obtain', 'Hillary', 'Clinton', 's', 'missing', 'emails', 'from', 'Russian', 'hackers', 'killed', 'himself', 'in', 'a', 'Minnesota', 'hotel', 'room', 'days', 'after', 'talking', 'to', 'The', 'Wall', 'Street', 'Journal', 'about', 'his', 'efforts,', 'public', 'records', 'show.In', 'a', 'room', 'at', 'a', 'Rochester', 'hotel', 'used', 'almost', 'exclusively', 'by', 'Mayo', 'Clinic', 'patients', 'and', 'relatives,', 'Peter', 'W.', 'Smith,', '81,', 'left', 'a', 'carefully', 'prepared', 'file', 'of', 'documents,', 'which', 'includes', 'a', 'statement', 'police', 'called', 'a', 'suicide', 'note', 'in', 'which', 'he', 'said', 'he', 'was', 'in', 'ill', 'health', 'and', 'a', 'life', 'insurance', 'policy', 'was', 'expiring.Days', 'earlier,', 'the', 'financier', 'from', 'suburban', 'Lake', 'Forest', 'gave', 'an', 'interview', 'to', 'the', 'Journal', 'about', 'his', 'quest,', 'and', 'it', 'published', 'stories', 'about', 'his', 'efforts', 'beginning', 'in', 'late', 'June.', 'The', 'Journal', 'also', 'reported', 'it', 'had', 'seen', 'emails', 'written', 'by', 'Smith', 'showing', 'his', 'team', 'considered', 'retired', 'Lt.', 'Gen.', 'Michael', 'Flynn,', 'then', 'a', 'top', 'adviser', 'to', 'Republican', 'Donald', 'Trump', 's', 'campaign,', 'as', 'an', 'ally.', 'Flynn', 'briefly', 'was', 'President', 'Trump', 's', 'national', 'security', 'adviser', 'and', 'resigned', 'after', 'it', 'was', 'determined', 'he', 'had', 'failed', 'to', 'disclose', 'contacts', 'with', 'Russia.At', 'the', 'time,', 'the', 'newspaper', 'reported', 'Smith', 's', 'May', '14', 'death', 'came', 'about', '10', 'days', 'after', 'he', 'granted', 'the', 'interview.', 'Mystery', 'shrouded', 'how', 'and', 'where', 'he', 'had', 'died,', 'but', 'the', 'lead', 'reporter', 'on', 'the', 'stories', 'said', 'on', 'a', 'podcast', 'he', 'had', 'no', 'reason', 'to', 'believe', 'the', 'death', 'was', 'the', 'result', 'of', 'foul', 'play', 'and', 'that', 'Smith', 'likely', 'had', 'died', 'of', 'natural', 'causes.The', 'Journal', 'stories', 'said', 'it', 'was', 'on', 'Labor', 'Day', 'weekend', 'in', '2016', 'that', 'Smith', 'had', 'assembled', 'a', 'team', 'to', 'acquire', 'emails', 'the', 'team', 'theorized', 'might', 'have', 'been', 'stolen', 'from', 'the', 'private', 'server', 'Clinton', 'had', 'used', 'while', 'secretary', 'of', 'state.', 'Smith', 's', 'focus', 'was', 'the', 'more', 'than', '30,000', 'emails', 'Clinton', 'said', 'she', 'deleted', 'because', 'they', 'related', 'to', 'personal', 'matters.', 'A', 'huge', 'cache', 'of', 'other', 'Clinton', 'emails', 'were', 'made', 'public.Smith', 'told', 'the', 'Journal', 'he', 'believed', 'the', 'missing', 'emails', 'might', 'have', 'had', 'been', 'obtained', 'by', 'Russian', 'hackers.', 'He', 'also', 'said', 'he', 'thought', 'the', 'correspondence', 'related', 'to', 'Clinton', 's', 'official', 'duties.', 'He', 'told', 'the', 'Journal', 'he', 'worked', 'independently', 'and', 'was', 'not', 'part', 'of', 'the', 'Trump', 'campaign.', 'He', 'also', 'told', 'the', 'Journal', 'he', 'and', 'his', 'team', 'found', 'five', 'groups', 'of', 'hackers', 'two', 'of', 'them', 'Russian', 'groups', 'who', 'claimed', 'to', 'have', 'Clinton', 's', 'missing', 'emails.Smith', 'had', 'a', 'history', 'of', 'doing', 'opposition', 'research,', 'the', 'formal', 'term', 'for', 'unflattering', 'information', 'that', 'political', 'operatives', 'dig', 'up', 'about', 'rival', 'candidates.For', 'years,', 'Democratic', 'President', 'Bill', 'Clinton', 'was', 'Smith', 's', 'target.', 'The', 'wealthy', 'businessman', 'had', 'a', 'hand', 'in', 'exposing', 'the', 'Troopergate', 'allegations', 'about', 'Bill', 'Clinton', 's', 'sex', 'life.', 'And', 'he', 'discussed', 'financing', 'a', 'probe', 'of', 'a', '1969', 'trip', 'Bill', 'Clinton', 'had', 'taken', 'while', 'in', 'college', 'to', 'the', 'Soviet', 'Union,', 'according', 'to', 'Salon', 'magazine.Smith', 's', 'death', 'occurred', 'at', 'the', 'Aspen', 'Suites', 'in', 'Rochester,', 'records', 'show.', 'They', 'list', 'the', 'cause', 'of', 'death', 'as', 'asphyxiation', 'due', 'to', 'displacement', 'of', 'oxygen', 'in', 'confined', 'space', 'with', 'helium.', 'Rochester', 'Police', 'Chief', 'Roger', 'Peterson', 'on', 'Wednesday', 'called', 'his', 'manner', 'of', 'death', 'unusual,', 'but', 'a', 'funeral', 'home', 'worker', 'said', 'he', 'd', 'seen', 'it', 'before.An', 'employee', 'with', 'Rochester', 'Cremation', 'Services,', 'the', 'funeral', 'home', 'that', 'responded', 'to', 'the', 'hotel,', 'said', 'he', 'helped', 'remove', 'Smith', 's', 'body', 'from', 'his', 'room', 'and', 'recalled', 'seeing', 'a', 'tank.However,', 'the', 'Chicago', 'Tribune', 'obtained', 'a', 'Minnesota', 'state', 'death', 'record', 'filed', 'in', 'Olmsted', 'County', 'that', 'says', 'Smith', 'committed', 'suicide', 'in', 'a', 'hotel', 'near', 'the', 'Mayo', 'Clinic', 'at', '1:17', 'p.m.', 'on', 'Sunday,', 'May', '14.', 'He', 'was', 'found', 'with', 'a', 'bag', 'over', 'his', 'head', 'with', 'a', 'source', 'of', 'helium', 'attached.', 'A', 'medical', 'examiner', 's', 'report', 'gives', 'the', 'same', 'account,', 'without', 'specifying', 'the', 'time,', 'and', 'a', 'report', 'from', 'Rochester', 'police', 'further', 'details', 'his', 'suicide.In', 'the', 'note', 'recovered', 'by', 'police,', 'Smith', 'apologized', 'to', 'authorities', 'and', 'said', 'that', 'NO', 'FOUL', 'PLAY', 'WHATSOEVER', 'was', 'involved', 'in', 'his', 'death.', 'He', 'wrote', 'that', 'he', 'was', 'taking', 'his', 'own', 'life', 'because', 'of', 'a', 'RECENT', 'BAD', 'TURN', 'IN', 'HEALTH', 'SINCE', 'JANUARY,', '2017', 'and', 'timing', 'related', 'TO', 'LIFE', 'INSURANCE', 'OF', '$5', 'MILLION', 'EXPIRING.', 'One', 'of', 'Smith', 's', 'former', 'employees', 'told', 'the', 'Tribune', 'he', 'thought', 'the', 'elderly', 'man', 'had', 'gone', 'to', 'the', 'famed', 'clinic', 'to', 'be', 'treated', 'for', 'a', 'heart', 'condition.', 'Mayo', 'spokeswoman', 'Ginger', 'Plumbo', 'said', 'Thursday', 'she', 'could', 'not', 'confirm', 'Smith', 'had', 'been', 'a', 'patient,', 'citing', 'medical', 'privacy', 'laws.Peter', 'Smith', 'wrote', 'two', 'blog', 'posts', 'dated', 'the', 'day', 'before', 'he', 'was', 'found', 'dead.', 'One', 'challenged', 'U.S.', 'intelligence', 'agency', 'findings', 'that', 'Russia', 'interfered', 'with', 'the', '2016', 'election.', 'Another', 'post', 'predicted:', 'As', 'attention', 'turns', 'to', 'international', 'affairs,', 'as', 'it', 'will', 'shortly,', 'the', 'Russian', 'interference', 'story', 'will', 'die', 'of', 'its', 'own', 'weight.', 'Chicago', 'Tribune', 'h/t', 'Gary', 'Klug', '@garyinlv01', 'Peter', 'W.', 'Smith', 'tweeted', 'an', 'interesting', 'article', 'on', 'January', '16,', '2017.', 'The', 'article', 'says', 'Russian', 'hackers', 'did', 'not', 'hack', 'the', 'DNC', 'emails', 'and', 'that', 'the', 'person', 'who', 'hacked', 'them', 'is', 'a', 'Russian', 'named', 'Dmitri', 'Alperovitch', 'who', 'also', 'worked', 'for', 'Barack', 'Obama.Russian', 'ex-national', 'guards', 'U.S.', 'nuclear', 'codes', 'for', 'Obama', 'https://t.co/j0vz8h1Cmx', 'Peter', 'W.', 'Smith', '(@PTRSIH)', 'January', '16,', '2017State', 'of', 'The', 'Nation', '2012', 'Russians', 'did', 'not', 'hack', 'the', 'DNC', 'system,', 'a', 'Russian', 'named', 'Dmitri', 'Alperovitch', 'is', 'the', 'hacker', 'and', 'he', 'works', 'for', 'President', 'Obama.', 'In', 'the', 'last', 'five', 'years', 'the', 'Obama', 'administration', 'has', 'turned', 'exclusively', 'to', 'one', 'Russian', 'to', 'solve', 'every', 'major', 'cyber-attack', 'in', 'America,', 'whether', 'the', 'attack', 'was', 'on', 'the', 'U.S.', 'government', 'or', 'a', 'corporation.', 'Only', 'one', 'super-hero', 'cyber-warrior', 'seems', 'to', 'have', 'the', 'codes', 'to', 'figure', 'out', 'if', 'a', 'system', 'was', 'hacked', 'and', 'by', 'whom.', 'Dmitri', 's', 'company,', 'CrowdStrike', 'has', 'been', 'called', 'in', 'by', 'Obama', 'to', 'solve', 'mysterious', 'attacks', 'on', 'many', 'high', 'level', 'government', 'agencies', 'and', 'American', 'corporations,', 'including:', 'German', 'Bundestag,', 'Democratic', 'National', 'Committee,', 'Democratic', 'Congressional', 'Campaign', 'Committee', '(DCCC),', 'the', 'White', 'House,', 'the', 'State', 'Department,', 'SONY,', 'and', 'many', 'others.CrowdStrike', 's', 'philosophy', 'is:', 'You', 'don', 't', 'have', 'a', 'malware', 'problem;', 'you', 'have', 'an', 'adversary', 'problem.', 'CrowdStrike', 'has', 'played', 'a', 'critical', 'role', 'in', 'the', 'development', 'of', 'America', 's', 'cyber-defense', 'policy.', 'Dmitri', 'Alperovitch', 'and', 'George', 'Kurtz,', 'a', 'former', 'head', 'of', 'the', 'FBI', 'cyberwarfare', 'unit', 'founded', 'CrowdStrike.', 'Shawn', 'Henry,', 'former', 'executive', 'assistant', 'director', 'at', 'the', 'FBI', 'is', 'now', 'CrowdStrike', 's', 'president', 'of', 'services.', 'The', 'company', 'is', 'crawling', 'with', 'former', 'U.S.', 'intelligence', 'agents.Before', 'Alperovitch', 'founded', 'CrowdStrike', 'in', '2011,', 'he', 'was', 'working', 'in', 'Atlanta', 'as', 'the', 'chief', 'threat', 'officer', 'at', 'the', 'antivirus', 'software', 'firm', 'McAfee,', 'owned', 'by', 'Intel', '(a', 'DARPA', 'company).', 'During', 'that', 'time,', 'he', 'discovered', 'the', 'Chinese', 'had', 'compromised', 'at', 'least', 'seventy-one', 'companies', 'and', 'organizations,', 'including', 'thirteen', 'defense', 'contractors,', 'three', 'electronics', 'firms,', 'and', 'the', 'International', 'Olympic', 'Committee.', 'He', 'was', 'the', 'only', 'person', 'to', 'notice', 'the', 'biggest', 'cyberattack', 'in', 'history!', 'Nothing', 'suspicious', 'about', 'that.Alperovitch', 'and', 'the', 'DNCAfter', 'CrowdStrike', 'was', 'hired', 'as', 'an', 'independent', 'vendor', 'by', 'the', 'DNC', 'to', 'investigate', 'a', 'possible', 'cyberattack', 'on', 'their', 'system,', 'Alperovitch', 'sent', 'the', 'DNC', 'a', 'proprietary', 'software', 'package', 'called', 'Falcon', 'that', 'monitors', 'the', 'networks', 'of', 'its', 'clients', 'in', 'real', 'time.', 'According', 'to', 'Alperovitch,', 'Falcon', 'lit', 'up,', 'within', 'ten', 'seconds', 'of', 'being', 'installed', 'at', 'the', 'DNC.', 'Alperovitch', 'had', 'his', 'proof', 'in', 'TEN', 'SECONDS', 'that', 'Russia', 'was', 'in', 'the', 'network.', 'This', 'alleged', 'evidence', 'of', 'Russian', 'hacking', 'has', 'yet', 'to', 'be', 'shared', 'with', 'anyone.As', 'Donald', 'Trump', 'has', 'pointed', 'out,', 'the', 'FBI,', 'the', 'agency', 'that', 'should', 'have', 'been', 'immediately', 'involved', 'in', 'hacking', 'that', 'effects', 'National', 'Security,', 'has', 'yet', 'to', 'even', 'examine', 'the', 'DNC', 'system', 'to', 'begin', 'an', 'investigation.', 'Instead,', 'the', 'FBI', 'and', '16', 'other', 'U.S.', 'intelligence', 'agencies', 'simply', 'agree', 'with', 'Obama', 's', 'most', 'trusted', 'cyberwarfare', 'expert', 'Dmitri', 'Alperovitch', 's', 'TEN', 'SECOND', 'assessment', 'that', 'produced', 'no', 'evidence', 'to', 'support', 'the', 'claim.Also', 'remember', 'that', 'it', 'is', 'only', 'Alperovitch', 'and', 'CrowdStrike', 'that', 'claim', 'to', 'have', 'evidence', 'that', 'it', 'was', 'Russian', 'hackers.', 'In', 'fact,', 'only', 'two', 'hackers', 'were', 'found', 'to', 'have', 'been', 'in', 'the', 'system', 'and', 'were', 'both', 'identified', 'by', 'Alperovitch', 'as', 'Russian', 'FSB', '(CIA)', 'and', 'the', 'Russian', 'GRU', '(DoD).', 'It', 'is', 'only', 'Alperovitch', 'who', 'claims', 'that', 'he', 'knows', 'that', 'it', 'is', 'Putin', 'behind', 'these', 'two', 'hackers.Alperovitch', 'failed', 'to', 'mention', 'in', 'his', 'conclusive', 'TEN', 'SECOND', 'assessment', 'that', 'Guccifer', '2.0', 'had', 'already', 'hacked', 'the', 'DNC', 'and', 'made', 'available', 'to', 'the', 'public', 'the', 'documents', 'he', 'hacked', 'before', 'Alperovitch', 'did', 'his', 'ten', 'second', 'assessment.', 'Alperovitch', 'reported', 'that', 'no', 'other', 'hackers', 'were', 'found,', 'ignoring', 'the', 'fact', 'that', 'Guccifer', '2.0', 'had', 'already', 'hacked', 'and', 'released', 'DNC', 'documents', 'to', 'the', 'public.', 'Alperovitch', 's', 'assessment', 'also', 'goes', 'directly', 'against', 'Julian', 'Assange', 's', 'repeated', 'statements', 'that', 'the', 'DNC', 'leaks', 'did', 'not', 'come', 'from', 'the', 'Russians.The', 'ridiculously', 'fake', 'cyber-attack', 'assessment', 'done', 'by', 'Alperovitch', 'and', 'CrowdStrike', 'na', 'vely', 'flies', 'in', 'the', 'face', 'of', 'the', 'fact', 'that', 'a', 'DNC', 'insider', 'admitted', 'that', 'he', 'had', 'released', 'the', 'DNC', 'documents.', 'Julian', 'Assange', 'implied', 'in', 'an', 'interview', 'that', 'the', 'murdered', 'Democratic', 'National', 'Committee', 'staffer,', 'Seth', 'Rich,', 'was', 'the', 'source', 'of', 'a', 'trove', 'of', 'damaging', 'emails', 'the', 'website', 'posted', 'just', 'days', 'before', 'the', 'party', 's', 'convention.', 'Seth', 'was', 'on', 'his', 'way', 'to', 'testify', 'about', 'the', 'DNC', 'leaks', 'to', 'the', 'FBI', 'when', 'he', 'was', 'shot', 'dead', 'in', 'the', 'street.It', 'is', 'also', 'absurd', 'to', 'hear', 'Alperovitch', 'state', 'that', 'the', 'Russian', 'FSB', '(equivalent', 'to', 'the', 'CIA)', 'had', 'been', 'monitoring', 'the', 'DNC', 'site', 'for', 'over', 'a', 'year', 'and', 'had', 'done', 'nothing.', 'No', 'attack,', 'no', 'theft,', 'and', 'no', 'harm', 'was', 'done', 'to', 'the', 'system', 'by', 'this', 'false-flag', 'cyber-attack', 'on', 'the', 'DNC', 'or', 'at', 'least,', 'Alperovitch', 'reported', 'there', 'was', 'an', 'attack.', 'The', 'second', 'hacker,', 'the', 'supposed', 'Russian', 'military', '(GRU', 'like', 'the', 'U.S.', 'DoD)', 'hacker,', 'had', 'just', 'entered', 'the', 'system', 'two', 'weeks', 'before', 'and', 'also', 'had', 'done', 'nothing', 'but', 'observe.It', 'is', 'only', 'Alperovitch', 's', 'word', 'that', 'reports', 'that', 'the', 'Russian', 'FSB', 'was', 'looking', 'for', 'files', 'on', 'Donald', 'Trump.', 'It', 'is', 'only', 'this', 'false', 'claim', 'that', 'spuriously', 'ties', 'Trump', 'to', 'the', 'alleged', 'attack.', 'It', 'is', 'also', 'only', 'Alperovitch', 'who', 'believes', 'that', 'this', 'hack', 'that', 'was', 'supposedly', 'looking', 'for', 'Trump', 'files', 'was', 'an', 'attempt', 'to', 'influence', 'the', 'election.', 'No', 'files', 'were', 'found', 'about', 'Trump', 'by', 'the', 'second', 'hacker,', 'as', 'we', 'know', 'from', 'Wikileaks', 'and', 'Guccifer', '2.0', 's', 'leaks.', 'To', 'confabulate', 'that', 'Russian', 's', 'hacked', 'the', 'DNC', 'to', 'influence', 'the', 'elections', 'is', 'the', 'claim', 'of', 'one', 'well-known', 'Russian', 'spy.', 'Then,', '17', 'U.S.', 'intelligence', 'agencies', 'unanimously', 'confirm', 'that', 'Alperovitch', 'is', 'correct', 'even', 'though', 'there', 'is', 'no', 'evidence', 'and', 'no', 'investigation', 'was', 'ever', 'conducted.How', 'does', 'Dmitri', 'Alperovitch', 'have', 'such', 'power?', 'Why', 'did', 'Obama', 'again', 'and', 'again', 'use', 'Alperovitch', 's', 'company,', 'CrowdStrike,', 'when', 'they', 'have', 'miserably', 'failed', 'to', 'stop', 'further', 'cyber-attacks', 'on', 'the', 'systems', 'they', 'were', 'hired', 'to', 'protect?', 'Why', 'should', 'anyone', 'believe', 'CrowdStrikes', 'false-flag', 'report?After', 'documents', 'from', 'the', 'DNC', 'continued', 'to', 'leak,', 'and', 'Guccifer', '2.0', 'and', 'Wikileaks', 'made', 'CrowdStrike', 's', 'report', 'look', 'foolish,', 'Alperovitch', 'decided', 'the', 'situation', 'was', 'far', 'worse', 'than', 'he', 'had', 'reported.', 'He', 'single-handedly', 'concluded', 'that', 'the', 'Russians', 'were', 'conducting', 'an', 'influence', 'operation', 'to', 'help', 'win', 'the', 'election', 'for', 'Trump.', 'This', 'false', 'assertion', 'had', 'absolutely', 'no', 'evidence', 'to', 'back', 'it', 'up.On', 'July', '22,', 'three', 'days', 'before', 'the', 'Democratic', 'convention', 'in', 'Philadelphia,', 'WikiLeaks', 'dumped', 'a', 'massive', 'cache', 'of', 'emails', 'that', 'had', 'been', 'stolen', '(not', 'hacked)', 'from', 'the', 'DNC.', 'Reporters', 'soon', 'found', 'emails', 'suggesting', 'that', 'the', 'DNC', 'leadership', 'had', 'favored', 'Hillary', 'Clinton', 'in', 'her', 'primary', 'race', 'against', 'Bernie', 'Sanders,', 'which', 'led', 'Debbie', 'Wasserman', 'Schultz,', 'the', 'DNC', 'chair,', 'along', 'with', 'three', 'other', 'officials,', 'to', 'resign.Just', 'days', 'later,', 'it', 'was', 'discovered', 'that', 'the', 'Democratic', 'Congressional', 'Campaign', 'Committee', '(DCCC)', 'had', 'been', 'hacked.', 'CrowdStrike', 'was', 'called', 'in', 'again', 'and', 'once', 'again,', 'Alperovitch', 'immediately', 'believed', 'that', 'Russia', 'was', 'responsible.', 'A', 'lawyer', 'for', 'the', 'DCCC', 'gave', 'Alperovitch', 'permission', 'to', 'confirm', 'the', 'leak', 'and', 'to', 'name', 'Russia', 'as', 'the', 'suspected', 'author.', 'Two', 'weeks', 'later,', 'files', 'from', 'the', 'DCCC', 'began', 'to', 'appear', 'on', 'Guccifer', '2.0', 's', 'website.', 'This', 'time', 'Guccifer', 'released', 'information', 'about', 'Democratic', 'congressional', 'candidates', 'who', 'were', 'running', 'close', 'races', 'in', 'Florida,', 'Ohio,', 'Illinois,', 'and', 'Pennsylvania.', 'On', 'August', '12,', 'Guccifer', 'went', 'further,', 'publishing', 'a', 'spreadsheet', 'that', 'included', 'the', 'personal', 'email', 'addresses', 'and', 'phone', 'numbers', 'of', 'nearly', 'two', 'hundred', 'Democratic', 'members', 'of', 'Congress.Once', 'again,', 'Guccifer', '2.0', 'proved', 'Alperovitch', 'and', 'CrowdStrike', 's', 'claims', 'to', 'be', 'grossly', 'incorrect', 'about', 'the', 'hack', 'originating', 'from', 'Russia,', 'with', 'Putin', 'masterminding', 'it', 'all.', 'Nancy', 'Pelosi', 'offered', 'members', 'of', 'Congress', 'Alperovitch', 's', 'suggestion', 'of', 'installing', 'Falcon,', 'the', 'system', 'that', 'failed', 'to', 'stop', 'cyberattacks', 'at', 'the', 'DNC,', 'on', 'all', 'congressional', 'laptops.Key', 'Point:', 'Once', 'Falcon', 'was', 'installed', 'on', 'the', 'computers', 'of', 'members', 'of', 'the', 'U.S.', 'Congress,', 'CrowdStrike', 'had', 'even', 'further', 'full', 'access', 'into', 'U.S.', 'government', 'accounts.Obama', 'No', 'Friend', 'of', 'AmericaObama', 'is', 'no', 'friend', 'of', 'America', 'in', 'the', 'war', 'against', 'cyber-attacks.', 'The', 'very', 'agencies', 'and', 'departments', 'being', 'defended', 'by', 'Michael', 'Alperovitch', 's', 'singular', 'and', 'most', 'brilliant', 'ability', 'to', 'write', 'encryption', 'codes', 'have', 'all', 'been', 'successfully', 'attacked', 'and', 'compromised', 'since', 'Michael', 'set', 'up', 'the', 'codes.', 'But', 'we', 'shouldn', 't', 'worry,', 'because', 'if', 'there', 'is', 'a', 'cyberattack', 'in', 'the', 'Obama', 'administration,', 'Michael', 's', 'son', 'Dmitri', 'is', 'called', 'in', 'to', 'prove', 'that', 'it', 'isn', 't', 'the', 'fault', 'of', 'his', 'father', 's', 'codes.', 'It', 'was', 'the', 'damn', 'Russians', ',', 'or', 'even', 'Putin', 'himself', 'who', 'attacked', 'American', 'networks.Not', 'one', 'of', 'the', '17', 'U.S.', 'intelligence', 'agencies', 'is', 'capable', 'of', 'figuring', 'out', 'a', 'successful', 'cyberattack', 'against', 'America', 'without', 'Michael', 'and', 'Dmitri', 's', 'help.', 'Those', 'same', '17', 'U.S.', 'intelligence', 'agencies', 'were', 'not', 'able', 'to', 'effectively', 'launch', 'a', 'successful', 'cyberattack', 'against', 'Russia.', 'It', 'seems', 'like', 'the', 'Russian', 's', 'have', 'strong', 'codes', 'and', 'America', 'has', 'weak', 'codes.', 'We', 'can', 'thank', 'Michael', 'and', 'Dmitri', 'Alperovitch', 'for', 'that.It', 'is', 'clear', 'that', 'there', 'was', 'no', 'DNC', 'hack', 'beyond', 'Guccifer', '2.0.', 'Dmitri', 'Alperovitch', 'is', 'a', 'frontman', 'for', 'his', 'father', 's', 'encryption', 'espionage', 'mission.Is', 'it', 'any', 'wonder', 'that', 'Trump', 'says', 'that', 'he', 'has', 'his', 'own', 'people', 'to', 'deliver', 'his', 'intelligence', 'to', 'him', 'that', 'is', 'outside', 'of', 'the', 'infiltrated', 'U.S.', 'government', 'intelligence', 'agencies', 'and', 'the', 'Obama', 'administration?', 'Isn', 't', 'any', 'wonder', 'that', 'citizens', 'have', 'to', 'go', 'anywhere', 'BUT', 'the', 'MSM', 'to', 'find', 'real', 'news', 'or', 'that', 'the', 'new', 'administration', 'has', 'to', 'go', 'to', 'independent', 'news', 'to', 'get', 'good', 'intel?It', 'is', 'hard', 'to', 'say', 'anything', 'more', 'damnable', 'than', 'to', 'again', 'quote', 'Dmitri', 'on', 'these', 'very', 'issues:', 'If', 'someone', 'steals', 'your', 'keys', 'to', 'encrypt', 'the', 'data,', 'it', 'doesn', 't', 'matter', 'how', 'secure', 'the', 'algorithms', 'are.', 'Dmitri', 'Alperovitch,', 'founder', 'of', 'CrowdStrikeFor', 'entire', 'story:', 'The', 'State', 'of', 'The', 'Nation', '2012'], tags=['Test_13397']),\n",
              " TaggedDocument(words=['During', 'a', 'United', 'Nations', 'Summit', 'in', 'New', 'York', 'City', 'today', 'focusing', 'on', 'women,', 'Canadian', 'Prime', 'Minister', 'Justin', 'Trudeau', 'explained', 'why', 'he', 'is', 'going', 'to', 'keep', 'saying', 'he', 'is', 'a', 'feminist.', 'I', 'm', 'going', 'to', 'keep', 'saying', 'loudly', 'and', 'clearly', 'that', 'I', 'am', 'a', 'feminist,', 'until', 'it', 'is', 'met', 'with', 'a', 'shrug,', 'said', 'Trudeau.', 'Why', 'does', 'every', 'time', 'I', 'say', 'I', 'm', 'a', 'feminist,', 'the', 'twitterverse', 'explodes', 'and', 'news', 'medias', 'pick', 'it', 'up.', 'It', 'shouldn', 't', 'be', 'something', 'that', 'creates', 'a', 'reaction.', 'It', 'simply', 'is', 'saying,', 'I', 'believe', 'in', 'the', 'equality', 'of', 'men', 'and', 'women', 'and', 'I', 'believe', 'that', 'we', 'still', 'have', 'an', 'awful', 'lot', 'of', 'work', 'to', 'do', 'to', 'get', 'there.', 'Trudeau', 'added,', 'It', 's', 'just', 'really,', 'really', 'obvious', 'that', 'we', 'should', 'be', 'standing', 'up', 'for', 'women', 's', 'rights', 'and', 'trying', 'to', 'create', 'more', 'equal', 'societies.', 'Like,', 'duh.', 'Trudeau', 'won', 'election', 'in', 'October', '2015', 'after', 'a', 'stunning', 'comeback', 'from', 'third', 'in', 'the', 'polls.', 'He', 'famously', 'appointed', 'an', 'equal', 'number', 'of', 'men', 'and', 'women', 'in', 'his', 'cabinet,', 'and', 'cited', 'his', 'reasoning', 'for', 'doing', 'so,', 'because', 'it', 's', '2015.', 'During', 'the', 'summit,', 'he', 'called', 'on', 'World', 'leaders', 'to', 'follow', 'Canada', 's', 'lead', 'when', 'appointing', 'their', 'own', 'cabinets.', 'Any', 'world', 'leaders', 'who', 'tell', 'me', 'I', 'd', 'love', 'to,', 'I', 'just', 'can', 't', 'do', 'that', 'with', 'the', 'current', 'configuration', 'of', 'our', 'parliament', 'or', 'of', 'my', 'party', ',', 'I', 'say:', 'Well,', 'what', 'are', 'you', 'doing', 'to', 'change', 'that', 'configuration', 'and', 'draw', 'out', 'those', 'extraordinary', 'women', 'who', 'can', 'be', 'leaders', 'that', 'we', 'need?', ',', 'he', 'told', 'the', 'crowd.His', 'party,', 'the', 'Liberal', 'Party,', 'also', 'recouped', 'a', 'majority', 'in', 'the', 'Canadian', 'Parliament.', 'His', 'election', 'also', 'inspired', 'an', 'increase', 'of', 'voter', 'turnout', 'by', '7.4', 'percent', 'to', '68.5', 'percent.', 'Trudeau', 'won', 'election', 'on', 'a', 'campaign', 'platform', 'of', 'progressive', 'reforms,', 'such', 'as', 'increasing', 'taxes', 'on', 'the', 'wealthy', 'and', 'dramatically', 'improving', 'infrastructure.', 'Trudeau', 'has', 'also', 'given', 'warm', 'welcomes', 'to', 'Syrian', 'refugees', 'arriving', 'in', 'Canada', 'to', 'seek', 'asylum.Just', 'last', 'week,', 'Trudeau', 'met', 'with', 'President', 'Barack', 'Obama', 'to', 'promote', 'ties', 'between', 'Canada', 'and', 'the', 'United', 'States,', 'and', 'to', 'announce', 'joint', 'efforts', 'to', 'combat', 'climate', 'change.', 'Trudeau', 's', 'predecessor,', 'Stephen', 'Harper,', 'as', 'an', 'aggressive', 'proponent', 'for', 'the', 'oil', 'industry', 'and', 'did', 'not', 'share', 'the', 'same', 'values', 'in', 'fighting', 'climate', 'change', 'Trudeau', 'and', 'Obama', 'do.', 'Featured', 'image', 'courtesy', 'of', 'Flickr'], tags=['Test_13398']),\n",
              " TaggedDocument(words=['Well,', 'well,', 'well', 'the', 'left', 'is', 'trying', 'pretty', 'much', 'anything', 'to', 'discount', 'the', 'Trump', 'candidacy.', 'WaPo', 'had', 'a', 'blurb', 'earlier', 'in', 'the', 'week', 'that', 'said', 'Bill', 'Clinton', 'suggested', 'Trump', 'run', 'for', 'president.', 'Nope,', 'nada,', 'did', 'NOT', 'happen!', 'We', 'can', 'put', 'this', 'lie', 'to', 'bed', 'Donald', 'Trump', 's', 'race', 'for', 'the', 'White', 'House', 'has', 'nothing', 'to', 'do', 'with', 'a', 'phone', 'call', 'with', 'Bill', 'Clinton,', 'according', 'to', 'a', 'Hillary', 'Clinton', 'campaign', 'spokeswoman.', 'They', 'exchanged', 'pleasantries.', 'Mr.', 'Trump', 'noted', 'that', 'he', 'was', 'thinking', 'about', 'running', 'for', 'president,', 'and', 'the', 'president', 'wished', 'him', 'well', 'but', 'didn', 't', 'give', 'him', 'any', 'advice,', 'Jen', 'Palmieri,', 'the', 'communications', 'director', 'for', 'Clinton', 's', 'campaign,', 'told', 'Andrea', 'Mitchell', 'of', 'MSNBC', 'on', 'Thursday.', 'Donald', 'Trump', 'doesn', 't', 'strike', 'me', 'as', 'somebody', 'who', 'takes', 'a', 'big', 'step', 'like', 'this', 'because', 'somebody', 'else', 'wants', 'him', 'to', 'do;', 'he', 'seems', 'to', 'be', 'somebody', 'who', 'very', 'much', 'operates', 'under', 'the', 'premise', 'of', 'free', 'will.', 'The', 'Clinton', 'campaign', 'played', 'no', 'behind-the-scenes', 'role', 'in', 'encouraging', 'Trump', 's', 'candidacy,', 'Palmieri', 'added.Earlier', 'this', 'week,', 'The', 'Washington', 'Post', 'reported', 'Trump', 'and', 'former', 'President', 'Clinton', 'spoke', 'on', 'the', 'phone', 'before', 'the', 'real', 'estate', 'mogul', 'announced', 'his', '2016', 'bid.Sources', 'in', 'the', 'story', 'said', 'that', 'Bill', 'Clinton', 'urged', 'Trump', 'to', 'play', 'a', 'larger', 'role', 'in', 'the', 'Republican', 'Party.', 'Trump', 'has', 'taken', 'the', 'lead', 'in', 'the', 'GOP', 'primary', 'in', 'several', 'polls', 'and', 'is', 'in', 'the', 'No.', '1', 'spot', 'entering', 'the', 'first', 'prime-time', 'debate', 'Thursday.Via:', 'Politico'], tags=['Test_13399']),\n",
              " TaggedDocument(words=['William', 'Stanford', 'Nye', 'or', 'Bill', 'Nye', 'is', 'an', 'American', 'comedian,', 'actor,', 'writer', 'and', 'science', 'educator.', 'Bill', 'Nye', 'initially', 'started', 'working', 'as', 'a', 'mechanical', 'engineer', 'and', 'later', 'turned', 'to', 'hosting', 'television', 'shows.', 'Bill', 'Nye', 'has', 'a', 'net', 'worth', 'of', '$6.5', 'Million.He', 'has', 'acquired', 'his', 'wealth', 'and', 'fame', 'mainly', 'because', 'of', 'the', 'television', 'show', 'Bill', 'Nye', 'the', 'Science', 'Guy', 'that', 'was', 'produced', 'jointly', 'by', 'Disney', 'and', 'PBS.', 'The', 'show', 'aired', 'for', 'almost', 'four', 'years.', 'It', 'was', 'aired', 'first', 'in', '1993,', 'and', 'it', 'continued', 'till', 'late', '1998.Nye', 'also', 'attended', 'Sidwell', 'Friends', 'in', 'DC,', 'the', 'same', 'elite', 'private', 'prep', 'school', 'as', 'the', 'Obama', 'girls.', 'Nye', 'was', 'honored', 'with', 'a', 'doctorate', 'by', 'the', 'Johns', 'Hopkins', 'University,', 'and', 'he', 'also', 'received', 'an', 'Honorary', 'Doctor', 'of', 'Science', 'Degree', 'in', '2011', 'from', 'the', 'Willamette', 'University.The', 'American', 'Mirror', 'Bill', 'Nye', 'the', 'science', 'guy', 'is', 'gaining', 'a', 'new', 'reputation', 'as', 'Bill', 'Nye', 'the', 'eugenics', 'guys', 'after', 'a', 'recent', 'episode', 'on', 'this', 'Netflix', 'show', 'Bill', 'Nye', 'Saves', 'The', 'World.', 'Nye', 'tackled', 'the', 'subject', 'of', 'overpopulation', 'during', 'the', 'show', 's', 'season', 'finale', 'on', 'Tuesday,', 'and', 'hosted', 'a', 'panel', 'discussion', 'that', 'included', 'the', 'question:', 'Should', 'we', 'have', 'policies', 'that', 'penalize', 'people', 'for', 'having', 'extra', 'is', 'in', 'the', 'developed', 'world?', 'Nye,', 'a', 'high-profile', 'climate', 'change', 'activist,', 'posed', 'the', 'question', 'after', 'panelist', 'Travis', 'Reider', 'of', 'Johns', 'Hopkins', 'University', 'alleged', 'that', 'the', 'average', 'Nigerian', 'emits', '.1', 'metric', 'tons', 'of', 'carbon', 'annually.Watch:https://youtu.be/eHmtn6gAioQThe', 'Nazi', 's', 'also', 'believed', 'in', 'killing', 'children', 'to', 'further', 'their', 'cause.', 'It', 's', 'actually', 'called', 'eugenics', ':', 'From', '1933', 'to', '1945,', 'Nazi', 'Germany', 'carried', 'out', 'a', 'campaign', 'to', 'cleanse', 'German', 'society', 'of', 'individuals', 'viewed', 'as', 'biological', 'threats', 'to', 'the', 'nation', 's', 'health.', 'Enlisting', 'the', 'help', 'of', 'physicians', 'and', 'medically', 'trained', 'geneticists,', 'psychiatrists,', 'and', 'anthropologists,', 'the', 'Nazis', 'developed', 'racial', 'health', 'policies', 'that', 'began', 'with', 'the', 'mass', 'sterilization', 'of', 'genetically', 'diseased', 'persons', 'and', 'ended', 'with', 'the', 'near', 'annihilation', 'of', 'European', 'Jewry.', 'With', 'the', 'patina', 'of', 'legitimacy', 'provided', 'by', 'racial', 'science', 'experts,', 'the', 'Nazi', 'regime', 'carried', 'out', 'a', 'program', 'of', 'approximately', '400,000', 'forced', 'sterilizations', 'and', 'over', '275,000', 'euthanasia', 'deaths', 'that', 'found', 'its', 'most', 'radical', 'manifestation', 'in', 'the', 'death', 'of', 'millions', 'of', 'racial', 'enemies', 'in', 'the', 'Holocaust.Rachel', 'Snow,', 'chief', 'of', 'population', 'development', 'for', 'the', 'United', 'Nations,', 'intervened.', 'I', 'would', 'take', 'issue', 'with', 'the', 'idea', 'that', 'we', 'do', 'anything', 'to', 'incentivize', 'fewer', 'children', 'or', 'more', 'child.', 'It', 's', 'justice', 'and', 'human', 'rights,', 'she', 'said.', 'People', 'should', 'have', 'the', 'number', 'of', 'children', 'they', 'want', 'and', 'the', 'timing', 'of', 'children', 'and', 'if', 'some', 'families', 'have', 'five', 'or', 'six', 'children,', 'God', 'bless', 'them,', 'I', 'mean,', 'that', 's', 'fine.', 'But', 'most', 'people', 'end', 'up', 'with', 'fewer.', 'Another', 'panelist,', 'Montefiore', 'Medical', 'Center', 'director', 'of', 'family', 'planning', 'Nerys', 'Benfield,', 'pointed', 'out', 'that', 'poor', 'and', 'minority', 'women', 'would', 'be', 'disproportionately', 'impacted', 'by', 'policies', 'that', 'penalize', 'parents', 'for', 'extra', 'kids.', 'These', 'people,', 'in', 'the', 'name', 'of', 'climate', 'change,', 'are', 'beginning', 'to', 'sound', 'an', 'awful', 'lot', 'like', 'the', 'eugenicists', 'of', 'the', 'early', '20th', 'century,', 'DanJanTube', 'posted', 'in', 'the', 'comments', 'of', 'a', 'YouTube', 'clip', 'of', 'the', 'show.', 'Disturbing.', 'Others', 'attacked', 'Nye', 'and', 'Netflix', 'on', 'Twitter.', 'This', 'might', 'be', 'the', 'darkest', 'show', 'Netflix', 'has', 'ever', 'streamed,', 'Chris', 'Morgan', 'tweeted.'], tags=['Test_13400']),\n",
              " TaggedDocument(words=['NEW', 'LONDON,', 'Conn.', '(Reuters)', '-', 'U.S.', 'President', 'Donald', 'Trump', 'said', 'on', 'Wednesday', 'he', 'would', 'use', 'his', 'upcoming', 'trip', 'to', 'Saudi', 'Arabia', 'to', 'challenge', 'the', 'leaders', 'of', 'Muslim', 'countries', 'to', '“fight', 'hatred', 'and', 'extremism”', 'while', 'pursuing', 'a', 'peaceful', 'future', 'for', 'their', 'faith.', 'Speaking', 'to', 'the', 'graduating', 'class', 'of', 'the', 'U.S.', 'Coast', 'Guard', 'Academy,', 'Trump', 'said', 'he', 'would', 'seek', 'new', 'partners', 'in', 'the', 'region', 'because', '“we', 'have', 'to', 'stop', 'radical', 'Islamic', 'terrorism.”'], tags=['Test_13401']),\n",
              " TaggedDocument(words=['(Reuters)', '-', 'Kansas', 'vowed', 'on', 'Wednesday', 'to', 'sue', 'the', 'Obama', 'administration', 'over', 'a', 'directive', 'telling', 'U.S.', 'public', 'schools', 'to', 'allow', 'transgender', 'students', 'to', 'use', 'bathrooms', 'and', 'locker', 'rooms', 'that', 'correspond', 'with', 'their', 'gender', 'identities.', 'The', 'announcement', 'by', 'Kansas', 'Attorney', 'General', 'Derek', 'Schmidt', 'made', 'the', 'state', 'the', '13th', 'to', 'wade', 'into', 'an', 'acrimonious', 'debate', 'about', 'the', 'rights', 'of', 'transgender', 'Americans.', 'It', 'comes', 'a', 'day', 'after', 'a', 'federal', 'appeals', 'court', 'refused', 'to', 'reconsider', 'its', 'ruling', 'that', 'gave', 'a', 'Virginia', 'transgender', 'high-school', 'student', 'access', 'to', 'the', 'bathroom', 'of', 'his', 'gender', 'identity.', 'The', 'court’s', 'decision', '“means', 'our', 'only', 'option', 'is', 'to', 'pursue', 'a', 'more', 'direct', 'challenge', 'to', 'the', 'Obama', 'administration’s', 'unlawful', 'efforts', 'to', 'unilaterally', 'rewrite', 'Title', 'IX,”', 'Schmidt,', 'a', 'Republican,', 'said', 'in', 'a', 'statement.', 'President', 'Barack', 'Obama’s', 'administration', 'on', 'May', '13', 'told', 'U.S.', 'public', 'schools', 'that', 'transgender', 'students', 'must', 'be', 'allowed', 'to', 'use', 'the', 'bathroom', 'of', 'their', 'choice,', 'upsetting', 'Republicans', 'and', 'paving', 'the', 'way', 'for', 'fights', 'over', 'federal', 'funding', 'and', 'legal', 'authority.', 'The', 'letter', 'relied', 'on', 'an', 'interpretation', 'of', 'Title', 'IX,', 'which', 'protects', 'people', 'from', 'discrimination', 'based', 'on', 'sex', 'in', 'education', 'initiatives', 'that', 'receive', 'federal', 'financial', 'assistance.', 'The', 'letter', 'did', 'not', 'have', 'the', 'force', 'of', 'law', 'but', 'contained', 'an', 'implicit', 'threat', 'that', 'schools', 'that', 'do', 'not', 'abide', 'by', 'the', 'directive', 'could', 'lose', 'federal', 'aid.', 'Schmidt', 'called', 'the', 'directive', 'an', 'attempt', 'to', 'expand', 'federal', 'power.', 'Also', 'on', 'Wednesday,', 'the', 'Kansas', 'Senate', 'passed', 'by', '30-to-8', 'a', 'resolution', 'opposing', 'the', 'Obama', 'administration’s', 'guidance', 'on', 'transgender', 'students’', 'bathroom', 'use', 'that', 'said', 'state', 'and', 'local', 'officials,', 'and', 'not', 'federal', 'officials,', 'should', 'decide', 'school', 'policy', 'and', 'that', 'the', 'directive', 'threatens', 'students’', 'privacy', 'and', 'safety.', 'Officials', 'from', '11', 'U.S.', 'states', '-', 'Alabama,', 'Arizona,', 'Georgia,', 'Louisiana,', 'Maine,', 'Oklahoma,', 'Tennessee,', 'Texas,', 'Utah,', 'West', 'Virginia,', 'Wisconsin', '-', 'sued', 'the', 'Obama', 'administration', 'last', 'week,', 'calling', 'its', 'directive', '“a', 'massive', 'social', 'experiment.”', 'Mississippi’s', 'Republican', 'governor', 'said', 'on', 'Thursday', 'he', 'planned', 'to', 'join', 'the', 'lawsuit.', 'Schmidt', 'said', 'he', 'is', 'reviewing', 'whether', 'Kansas', 'will', 'join', 'the', 'lawsuit', 'by', 'the', '11', 'states,', 'led', 'by', 'Texas,', 'or', 'file', 'a', 'separate,', 'similar', 'lawsuit.', 'North', 'Carolina', 'sued', 'the', 'federal', 'government', 'last', 'month,', 'seeking', 'to', 'protect', 'a', 'state', 'law', 'passed', 'in', 'March', 'that', 'bans', 'people', 'from', 'using', 'public', 'restrooms', 'not', 'corresponding', 'to', 'their', 'sex', 'assigned', 'at', 'birth.', 'But', 'while', 'many', 'states', 'have', 'protested', 'the', 'Obama', 'administration', 'directive,', 'others', 'have', 'acted', 'to', 'protect', 'the', 'rights', 'of', 'transgender', 'individuals.', 'Massachusetts', 'lawmakers', 'on', 'Wednesday', 'were', 'voting', 'on', 'a', 'measure', 'that', 'would', 'make', 'the', 'state', 'the', '18th', 'to', 'ban', 'discrimination', 'based', 'on', 'gender', 'identity.'], tags=['Test_13402']),\n",
              " TaggedDocument(words=['MADRID', '(Reuters)', '-', 'Catalan', 'leader', 'Carles', 'Puigdemont', 'should', 'come', 'back', 'to', 'the', 'path', 'of', 'the', 'law', 'if', 'he', 'wants', 'talks', 'to', 'take', 'place', 'and', 'he', 'has', 'no', 'right', 'to', 'impose', 'a', 'mediation', 'with', 'the', 'government,', 'Spain', 's', 'Deputy', 'Prime', 'Minister', 'Soraya', 'Saenz', 'de', 'Santamaria', 'said', 'on', 'Tuesday.', 'Neither', 'Mr.', 'Puigdemont', 'nor', 'anybody', 'else', 'can', 'claim', '...', 'to', 'impose', 'mediation.', 'Any', 'dialogue', 'between', 'democrats', 'has', 'to', 'take', 'place', 'within', 'the', 'law,', 'Saenz', 'de', 'Santamaria', 'said', 'after', 'Puigdemont', 'declared', 'Catalonia', 's', 'independence', 'from', 'Spain', 'but', 'immediately', 'suspended', 'it', 'to', 'allow', 'time', 'for', 'a', 'mediated', 'solution', 'with', 'Spain.', 'The', 'Spanish', 'government', 'will', 'meet', 'on', 'Wednesday', 'to', 'decide', 'on', 'its', 'response', 'to', 'this', 'declaration.'], tags=['Test_13403']),\n",
              " TaggedDocument(words=['At', 'the', 'start', 'of', 'the', 'Fox', 'News', 'Greg', 'Gutfeld', 'Show,', 'Gutfeld', 'showed', 'a', 'hilarious', 'clip', 'mocking', 'the', 'hysteria', 'of', 'the', 'Democrat', 'Party.', 'Gutfeld', 'began', 'his', 'show', 'by', 'telling', 'his', 'viewers,', 'I', 'm', 'really', 'excited', 'to', 'introduce', 'a', 'new', 'sponsor', 'to', 'our', 'show.', 'It', 's', 'a', 'new', 'drug.', 'It', 's', 'called', \"Victima'\", 'Gutfeld', 'went', 'on', 'to', 'explain', 'that', 'the', 'drug', 'was', 'created', 'to', 'help', 'Democrats', 'who', 'are', 'feeling', 'defeated', 'after', 'the', 'brutal', 'election', 'of', 'Donald', 'Trump', 'cope:', 'When', 'identity', 'politics', 'no', 'longer', 'work,', 'try', 'Victima', '.America', 's', 'been', 'witnessing', 'the', 'embarrassing', 'meltdown', 'of', 'liberals', 'ever', 'since', 'President', 'Trump', 'shocked', 'the', 'world', 'with', 'his', 'stinging', 'defeat', 'of', 'Crooked', 'Hillary,', 'so', 'Gutfeld', 'decided', 'to', 'have', 'a', 'little', 'fun', 'by', 'mocking', 'the', 'embarrassing', 'behavior', 'of', 'the', 'left.The', 'voice', 'in', 'the', 'commercial', 'mimics', 'the', 'typical', 'TV', 'drug', 'commercials', 'with', 'sappy', 'music', 'playing', 'in', 'the', 'background:', 'It', 's', 'not', 'what', 'it', 'used', 'to', 'be.', 'It', 'used', 'to', 'be', 'fine.The', 'Democratic', 'Party', 'it', 's', 'not', 'performong.It', 's', 'slow,', 'sluggish,', 'confused,', 'prone', 'to', 'fits', 'of', 'despair.', 'It', 'may', 'be', 'time', 'for', 'Victima', '.', 'Victims,', 'when', 'your', 'old', 'identity', 'politics', 'no', 'longer', 'work,', 'Victima', 'gives', 'you', 'that', 'added', 'boost', 'to', 'get', 'you', 'to', 'where', 'you', 'want', 'to', 'be.', 'When', 'I', 'take', 'Victima,', 'I', 'see', 'everyone', 'as', 'a', 'racist', '.', 'It', 'really', 'works.', 'Watch:Ironically,', 'FOX', 'News', 'fired', 'Bob', 'Beckel,', 'the', 'only', 'liberal', 'on', 'the', 'Gutfeld', 'Show', 'the', 'day', 'after', 'this', 'show', 'aired,', 'after', 'he', 'allegedly', 'made', 'a', 'racist', 'remark', 'that', 'offended', 'a', 'black', 'employee', 'working', 'at', 'FOX.', 'Hmmm'], tags=['Test_13404']),\n",
              " TaggedDocument(words=['HELSINKI', '(Reuters)', '-', 'Finnish', 'government', 'should', 'actively', 'push', 'the', 'European', 'Union', 'to', 'abolish', 'its', 'directive', 'on', 'daylight', 'saving', 'time,', 'a', 'parliament', 'committee', 'said', 'on', 'Thursday.', 'The', 'parliamentary', 'transport', 'and', 'communications', 'committee', 'gave', 'its', 'recommendation', 'to', 'the', 'government', 'after', 'more', 'than', '70,000', 'Finns', 'signed', 'a', 'citizens', 'petition', 'asking', 'the', 'state', 'to', 'give', 'up', 'the', 'practice.', 'Under', 'daylight', 'saving', 'time', 'clocks', 'move', 'forward', 'by', 'one', 'hour', 'during', 'summer', 'months', 'so', 'that', 'daylight', 'lasts', 'longer', 'into', 'evening.', 'Most', 'of', 'North', 'America', 'and', 'Europe', 'follows', 'the', 'custom,', 'while', 'the', 'majority', 'of', 'countries', 'elsewhere', 'do', 'not.', 'The', 'committee', 'said', 'that', 'after', 'hearing', 'several', 'experts,', 'it', 'concluded', 'that', 'people', 'do', 'not', 'adapt', 'smoothly', 'to', 'the', 'changes.', 'It', 'added', 'that', 'turning', 'the', 'clocks', 'caused', 'short-term', 'sleeping', 'disorders,', 'reduced', 'performance', 'at', 'work', 'and', 'could', 'also', 'lead', 'to', 'serious', 'health', 'problems.', 'The', 'primary', 'objective', 'should', 'be', 'to', 'abolish', 'the', 'clock', 'movements', 'on', 'uniform', 'basis', 'throughout', 'the', 'European', 'Union,', 'the', 'committee', 'said', 'in', 'a', 'statement.'], tags=['Test_13405']),\n",
              " TaggedDocument(words=['Another', 'conservative', 'Christian', 'pastor', 'is', 'praising', 'the', 'killing', 'of', '49', 'people', 'who', 'were', 'shot', 'at', 'an', 'Orlando', 'gay', 'nightclub', 'last', 'weekend.In', 'response', 'to', 'Sacramento', 'Pastor', 'Roger', 'Jimenez', 'lamenting', 'that', 'the', 'Orlando', 'shooter', 'didn', 't', 'kill', 'every', 'last', 'person', 'in', 'the', 'club', 'that', 'night', 'and', 'calling', 'for', 'the', 'government', 'to', 'line', 'every', 'gay', 'person', 'in', 'America', 'against', 'a', 'wall', 'and', 'execute', 'them', 'via', 'firing', 'squad,', 'Fort', 'Worth', 'Pastor', 'Donnie', 'Romero', 'of', 'the', 'Stedfast', 'Baptist', 'Church', 'applauded', 'the', 'hateful', 'sermon', 'and', 'added', 'his', 'sickening', 'two', 'cents.Romero', 'claimed', 'that', 'all', 'gay', 'people', 'are', 'pedophiles', 'even', 'though', 'that', 's', 'a', 'complete', 'and', 'utter', 'lie,', 'and', 'he', 'asked', 'God', 'to', 'finish', 'the', 'job', 'and', 'kill', 'any', 'of', 'the', 'wounded', 'victims', 'who', 'are', 'still', 'in', 'the', 'hospital.', 'These', '50', 'Sodomites', 'were', 'all', 'perverts', 'and', 'pedophiles', 'and', 'they', 'are', 'the', 'scum', 'of', 'the', 'earth', 'and', 'the', 'earth', 'is', 'a', 'little', 'bit', 'better', 'place', 'now,', 'and', 'I', 'll', 'even', 'take', 'it', 'a', 'little', 'further', 'I', 'heard', 'on', 'the', 'news', 'today', 'that', 'there', 'are', 'still', 'several', 'dozen', 'of', 'these', 'q****s', 'in', 'ICU', 'and', 'I', 'will', 'pray', 'that', 'God', 'will', 'finish', 'the', 'job', 'that', 'that', 'man', 'started,', 'that', 'he', 'will', 'end', 'their', 'lives', 'and', 'by', 'tomorrow', 'morning', 'they', 'will', 'all', 'be', 'burning', 'in', 'hell', 'so', 'that', 'they', 'don', 't', 'get', 'any', 'more', 'opportunities', 'to', 'go', 'out', 'and', 'hurt', 'little', 'children.', 'Here', 's', 'the', 'video', 'of', 'the', 'full', 'sermon', 'via', 'YouTube', 'if', 'you', 'can', 'stomach', 'it.', 'The', 'remarks', 'above', 'are', 'at', 'the', 'beginning', 'of', 'the', 'video.Romero', 'was', 'also', 'echoing', 'Arizona', 'pastor', 'Steven', 'Anderson,', 'who', 'also', 'had', 'no', 'sympathy', 'or', 'compassion', 'for', 'his', 'fellow', 'human', 'beings', 'and', 'also', 'called', 'for', 'all', 'gays', 'people', 'to', 'be', 'executed.In', 'an', 'interview', 'with', 'Fox', '4', 'about', 'his', 'hateful', 'remarks,', 'Romero', 'showed', 'no', 'regret', 'whatsoever.', 'The', 'Bible', 'teaches', 'they', 'are', 'predators,', 'and', 'I', 'believe', 'that', 'every', 'Sodomite', 'is', 'a', 'pedophile', 'and', 'is', 'a', 'predator,', 'Romero', 'said.But', 'Fox', '4', 'also', 'consulted', 'a', 'Bible', 'scholar,', 'who', 'made', 'it', 'clear', 'that', 'Romero', 'knows', 'nothing', 'about', 'the', 'book', 'or', 'Christianity.', 'No', 'Christian', 'who', 'has', 'any', 'formal', 'understanding', 'of', 'the', 'interaction', 'between', 'the', 'Old', 'and', 'the', 'New', 'Testament', 'would', 'say', 'such', 'a', 'thing,', 'said', 'theologian', 'Steve', 'Kellmeyer,', 'He', 's', 'just', 'one', 'more', 'in', 'a', 'long', 'list', 'of', 'people', 'who', 'don', 't', 'get', 'what', 'it', 'means', 'to', 'be', 'Christian.', 'It', 's', 'disgusting', 'how', 'so-called', 'men', 'of', 'God', 'have', 'been', 'reacting', 'to', 'this', 'tragedy', 'all', 'week', 'long', 'since', 'it', 'happened.', 'The', 'way', 'they', 'are', 'cheering', 'this', 'loss', 'of', 'life', 'makes', 'them', 'no', 'better', 'than', 'ISIS,', 'whose', 'leaders', 'we', 'all', 'know', 'must', 'be', 'praising', 'this', 'attack', 'just', 'as', 'fervently.', 'These', 'people', 'who', 'dare', 'to', 'call', 'themselves', 'Christians', 'have', 'thrown', 'in', 'with', 'the', 'terrorists', 'against', 'American', 'citizens', 'and', 'that', 'should', 'not', 'go', 'unpunished.', 'Each', 'of', 'these', 'churches', 'should', 'lose', 'their', 'tax', 'exemptions', 'and', 'be', 'shut', 'down', 'for', 'inciting', 'hate', 'crimes', 'against', 'the', 'LGBT', 'community.And', 'if', 'conservatives', 'are', 'still', 'wondering', 'why', 'more', 'people', 'are', 'rejecting', 'religion', 'today,', 'they', 'should', 'look', 'no', 'further', 'than', 'these', 'fundamentalist', 'pastors', 'they', 'have', 'allowed', 'to', 'lace', 'Christianity', 'with', 'hate', 'and', 'violence.Featured', 'image', 'via', 'screen', 'capture'], tags=['Test_13406'])]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvWLPOJODnz6",
        "outputId": "730ba07a-dc0a-4958-90a2-7eb077daecb3"
      },
      "source": [
        "model_dbow = doc2vec.Doc2Vec(dm=1, vector_size=350, negative=8, min_count=1, alpha=0.065, \n",
        "                     min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2300122.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hQ7zlMDDvmE",
        "outputId": "abb54a9b-c367-4aab-e361-6b5595c0f80a"
      },
      "source": [
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), \n",
        "                     total_examples=len(all_data), \n",
        "                     epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44689/44689 [00:00<00:00, 2078063.52it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 1931825.69it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2433991.50it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2812798.28it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2304675.41it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2422008.68it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2891420.90it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2724684.95it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2762472.02it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2348951.11it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2174570.18it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 1877678.45it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2231818.20it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2768551.64it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2096917.39it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2659165.41it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2028870.73it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2061720.43it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2179044.76it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2099830.30it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2915844.80it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2907612.68it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2324078.45it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2323761.52it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2143387.67it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2091185.74it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2249846.98it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2138667.68it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 2133361.23it/s]\n",
            "100%|██████████| 44689/44689 [00:00<00:00, 1477773.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PfwTT1jDyxc"
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agNgNUf5D1Rp"
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 350, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 350, 'Test')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR-zi0NhD3wC",
        "outputId": "548cb1e3-03ab-4941-d5d2-73a9c25ccc7a"
      },
      "source": [
        "model = SVC()\n",
        "model.fit(train_vectors_dbow, y_train)\n",
        "\n",
        "y_pred = model.predict(test_vectors_dbow)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9906019243678675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.99      0.99      0.99      7036\n",
            "        True       0.99      0.99      0.99      6371\n",
            "\n",
            "    accuracy                           0.99     13407\n",
            "   macro avg       0.99      0.99      0.99     13407\n",
            "weighted avg       0.99      0.99      0.99     13407\n",
            "\n"
          ]
        }
      ]
    }
  ]
}